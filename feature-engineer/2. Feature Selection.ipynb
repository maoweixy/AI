{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering Notebook Two: Feature Selection**  \n",
    "*Author: Yingxiang Chen, Zihan Yang*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference**\n",
    "- https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "- https://machinelearningmastery.com/an-introduction-to-feature-selection/\n",
    "- https://dcor.readthedocs.io/en/latest/functions/dcor.independence.distance_covariance_test.html\n",
    "- https://en.wikipedia.org/wiki/Distance_correlation\n",
    "- https://stats.stackexchange.com/questions/56881/whats-the-relationship-between-r2-and-f-test\n",
    "- https://en.wikipedia.org/wiki/Mutual_information\n",
    "- https://libguides.library.kent.edu/SPSS/ChiSquare\n",
    "- https://chrisalbon.com/machine_learning/feature_selection/anova_f-value_for_feature_selection/\n",
    "- https://online.stat.psu.edu/stat414/node/218/\n",
    "- http://featureselection.asu.edu/algorithms.php\n",
    "- https://en.wikipedia.org/wiki/Minimum_redundancy_feature_selection\n",
    "- Peng, H., Long, F., & Ding, C. (2005). Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy. IEEE Transactions on Pattern Analysis & Machine Intelligence, (8), 1226-1238.\n",
    "- Hall, M. A., & Smith, L. A. (1999, May). Feature selection for machine learning: comparing a correlation-based filter approach to the wrapper. In FLAIRS conference (Vol. 1999, pp. 235-239).\n",
    "- Yu, L., & Liu, H. (2003). Feature selection for high-dimensional data: A fast correlation-based filter solution. In Proceedings of the 20th international conference on machine learning (ICML-03) (pp. 856-863).\n",
    "- Zhao, Z., & Liu, H. (2007, June). Spectral feature selection for supervised and unsupervised learning. In Proceedings of the 24th international conference on Machine learning (pp. 1151-1157). ACM.\n",
    "- Robnik-Å ikonja, M., & Kononenko, I. (2003). Theoretical and empirical analysis of ReliefF and RReliefF. Machine learning, 53(1-2), 23-69.\n",
    "- https://machinelearningmastery.com/an-introduction-to-feature-selection/\n",
    "- http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Feature-Selection\" data-toc-modified-id=\"Feature-Selection-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Feature Selection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Filter-Methods\" data-toc-modified-id=\"Filter-Methods-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Filter Methods</a></span><ul class=\"toc-item\"><li><span><a href=\"#Univariate-Filter-Methods\" data-toc-modified-id=\"Univariate-Filter-Methods-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Univariate Filter Methods</a></span><ul class=\"toc-item\"><li><span><a href=\"#Variance-Threshold\" data-toc-modified-id=\"Variance-Threshold-1.1.1.1\"><span class=\"toc-item-num\">1.1.1.1&nbsp;&nbsp;</span>Variance Threshold</a></span></li><li><span><a href=\"#Pearson-Correlation-(regression-problem)\" data-toc-modified-id=\"Pearson-Correlation-(regression-problem)-1.1.1.2\"><span class=\"toc-item-num\">1.1.1.2&nbsp;&nbsp;</span>Pearson Correlation (regression problem)</a></span></li><li><span><a href=\"#Distance-Correlation-(regression-problem)\" data-toc-modified-id=\"Distance-Correlation-(regression-problem)-1.1.1.3\"><span class=\"toc-item-num\">1.1.1.3&nbsp;&nbsp;</span>Distance Correlation (regression problem)</a></span></li><li><span><a href=\"#F-Score-(regression-problem)\" data-toc-modified-id=\"F-Score-(regression-problem)-1.1.1.4\"><span class=\"toc-item-num\">1.1.1.4&nbsp;&nbsp;</span>F-Score (regression problem)</a></span></li><li><span><a href=\"#Mutual-Information-(regression-problem)\" data-toc-modified-id=\"Mutual-Information-(regression-problem)-1.1.1.5\"><span class=\"toc-item-num\">1.1.1.5&nbsp;&nbsp;</span>Mutual Information (regression problem)</a></span></li><li><span><a href=\"#Chi-squared-Statistics-(classification-problem)\" data-toc-modified-id=\"Chi-squared-Statistics-(classification-problem)-1.1.1.6\"><span class=\"toc-item-num\">1.1.1.6&nbsp;&nbsp;</span>Chi-squared Statistics (classification problem)</a></span></li><li><span><a href=\"#F-Score-(classification-problem)\" data-toc-modified-id=\"F-Score-(classification-problem)-1.1.1.7\"><span class=\"toc-item-num\">1.1.1.7&nbsp;&nbsp;</span>F-Score (classification problem)</a></span></li><li><span><a href=\"#Mutual-Information-(classification-problem)\" data-toc-modified-id=\"Mutual-Information-(classification-problem)-1.1.1.8\"><span class=\"toc-item-num\">1.1.1.8&nbsp;&nbsp;</span>Mutual Information (classification problem)</a></span></li></ul></li><li><span><a href=\"#Multivariate-Filter-Methods\" data-toc-modified-id=\"Multivariate-Filter-Methods-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Multivariate Filter Methods</a></span><ul class=\"toc-item\"><li><span><a href=\"#Max-Relevance-Min-Redundancy-(mRMR)\" data-toc-modified-id=\"Max-Relevance-Min-Redundancy-(mRMR)-1.1.2.1\"><span class=\"toc-item-num\">1.1.2.1&nbsp;&nbsp;</span>Max-Relevance Min-Redundancy (mRMR)</a></span></li><li><span><a href=\"#Correlation-based-Feature-Selection-(CFS)\" data-toc-modified-id=\"Correlation-based-Feature-Selection-(CFS)-1.1.2.2\"><span class=\"toc-item-num\">1.1.2.2&nbsp;&nbsp;</span>Correlation-based Feature Selection (CFS)</a></span></li><li><span><a href=\"#Fast-Correlation-based-Filter-(FCBF)\" data-toc-modified-id=\"Fast-Correlation-based-Filter-(FCBF)-1.1.2.3\"><span class=\"toc-item-num\">1.1.2.3&nbsp;&nbsp;</span>Fast Correlation-based Filter (FCBF)</a></span></li><li><span><a href=\"#ReliefF\" data-toc-modified-id=\"ReliefF-1.1.2.4\"><span class=\"toc-item-num\">1.1.2.4&nbsp;&nbsp;</span>ReliefF</a></span></li><li><span><a href=\"#Spectral-Feature-Selection-(SPEC)\" data-toc-modified-id=\"Spectral-Feature-Selection-(SPEC)-1.1.2.5\"><span class=\"toc-item-num\">1.1.2.5&nbsp;&nbsp;</span>Spectral Feature Selection (SPEC)</a></span></li></ul></li></ul></li><li><span><a href=\"#Wrapper-Methods\" data-toc-modified-id=\"Wrapper-Methods-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Wrapper Methods</a></span><ul class=\"toc-item\"><li><span><a href=\"#Deterministic-Algorithms\" data-toc-modified-id=\"Deterministic-Algorithms-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Deterministic Algorithms</a></span><ul class=\"toc-item\"><li><span><a href=\"#Recursive-Feature-Elimination-(SBS)\" data-toc-modified-id=\"Recursive-Feature-Elimination-(SBS)-1.2.1.1\"><span class=\"toc-item-num\">1.2.1.1&nbsp;&nbsp;</span>Recursive Feature Elimination (SBS)</a></span></li></ul></li><li><span><a href=\"#Randomized-Algorithms\" data-toc-modified-id=\"Randomized-Algorithms-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Randomized Algorithms</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simulated-Annealing-(SA)\" data-toc-modified-id=\"Simulated-Annealing-(SA)-1.2.2.1\"><span class=\"toc-item-num\">1.2.2.1&nbsp;&nbsp;</span>Simulated Annealing (SA)</a></span></li><li><span><a href=\"#Genetic-Algorithm-(GA)\" data-toc-modified-id=\"Genetic-Algorithm-(GA)-1.2.2.2\"><span class=\"toc-item-num\">1.2.2.2&nbsp;&nbsp;</span>Genetic Algorithm (GA)</a></span></li></ul></li></ul></li><li><span><a href=\"#Embedded-Methods\" data-toc-modified-id=\"Embedded-Methods-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Embedded Methods</a></span><ul class=\"toc-item\"><li><span><a href=\"#Regulization-Based-Methods\" data-toc-modified-id=\"Regulization-Based-Methods-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Regulization Based Methods</a></span><ul class=\"toc-item\"><li><span><a href=\"#Lasso-Regression-(Linear-Regression-with-L1-Norm)\" data-toc-modified-id=\"Lasso-Regression-(Linear-Regression-with-L1-Norm)-1.3.1.1\"><span class=\"toc-item-num\">1.3.1.1&nbsp;&nbsp;</span>Lasso Regression (Linear Regression with L1 Norm)</a></span></li><li><span><a href=\"#Logistic-Regression-(with-L1-Norm)\" data-toc-modified-id=\"Logistic-Regression-(with-L1-Norm)-1.3.1.2\"><span class=\"toc-item-num\">1.3.1.2&nbsp;&nbsp;</span>Logistic Regression (with L1 Norm)</a></span></li><li><span><a href=\"#LinearSVR/-LinearSVC\" data-toc-modified-id=\"LinearSVR/-LinearSVC-1.3.1.3\"><span class=\"toc-item-num\">1.3.1.3&nbsp;&nbsp;</span>LinearSVR/ LinearSVC</a></span></li></ul></li><li><span><a href=\"#Tree-Based-Methods\" data-toc-modified-id=\"Tree-Based-Methods-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Tree Based Methods</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After data preprocessing, we have generated a large number of new variables (recall by one-hot encoding, a large number of variables are generated and contain only 0 or 1). But in fact, some of the newly generated variables may be redundant. On the one hand, they might not contain any useful information and therefore cannot improve model performance. On the other hand, these extra variables will consume a lot of memory and computing power when building the model. Therefore, we should perform feature selection and select a subset of features for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filter method uses some statistics measures or hypothesis test results to assign scores to each feature. Features with higher scores tend to be more important and should be included in the subsets. Below is a sample ML pipeline based on train-validation-testset split method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T23:03:05.919797Z",
     "start_time": "2020-01-31T23:03:05.805176Z"
    }
   },
   "source": [
    "![image](./images/Filter_Pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Filter Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate filter methods select the best feature based on univariate statistical tests. It considers the feature independently or with regard to the target variable only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance approach simply removes all the features that have variance below a certain threshold. For example, a feature with zero variance (features that have the same value in all observations) should be removed because this feature can not examine any variance in the target variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:13.868357Z",
     "start_time": "2020-03-15T22:23:12.426977Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# create some synthesized dataset\n",
    "train_set = np.array([[1,2,3],[1,4,7],[1,4,9]]) # the first feature have zero variance\n",
    "# array([[1, 2, 3],\n",
    "#        [1, 4, 7],\n",
    "#        [1, 4, 9]])\n",
    "\n",
    "test_set = np.array([[3,2,3],[1,2,7]]) # the second feature have zero variance\n",
    "# array([[3, 2, 3],\n",
    "#        [1, 2, 7]])\n",
    "\n",
    "selector = VarianceThreshold()\n",
    "selector.fit(train_set) # fit on trainset\n",
    "transformed_train = selector.transform(train_set) # transform train set\n",
    "# the first feature has been removed\n",
    "# array([[2, 3],\n",
    "#        [4, 7],\n",
    "#        [4, 9]])\n",
    "\n",
    "transformed_test = selector.transform(test_set) # transform test set\n",
    "# array([[2, 3],\n",
    "#        [2, 7]])\n",
    "# although in the test set the second features has zero variance\n",
    "# but according to train set, we should remove the first feature only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T17:20:24.634631Z",
     "start_time": "2020-01-13T17:20:24.629774Z"
    }
   },
   "source": [
    "#### Pearson Correlation (regression problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson Correlation is generally used to measure the linear correlection between two **continuous** features. It can also measure the linear correlation between binary features and the target variable. And the categorical variable can be converted into binary features by one-hot encoding.   \n",
    "\n",
    "Formula:  \n",
    "  \n",
    "$r = \\frac{\\sum_{i=1}^{n}(X_i-\\bar{X})(Y_i-\\bar{Y})}{\\sqrt{(X_i-\\bar{X})^2}\\sqrt{(Y_i-\\bar{Y})^2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:13.933447Z",
     "start_time": "2020-03-15T22:23:13.870391Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# load dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "dataset = fetch_california_housing()\n",
    "X, y = dataset.data, dataset.target # use california_housing dataset as example\n",
    "# in this dataset, both X and y are continuous, so we can use Pearson Correlation to select features\n",
    "\n",
    "# use the first 15000 obs as train_set\n",
    "# the rest obs as test_set\n",
    "train_set = X[0:15000,:]\n",
    "test_set = X[15000:,]\n",
    "train_y = y[0:15000]\n",
    "\n",
    "# there is no pre-built Pearson function in sklearn\n",
    "# we need to use scipy.stats.pearsonr (can only compute the pearsonr between two features) \n",
    "# to create a multivariate version of Pearson function that can be used in SelectKBest\n",
    "\n",
    "def udf_pearsonr(X, y):\n",
    "    result = np.array([pearsonr(x, y) for x in X.T]) # list of (pearsonr, p-value)\n",
    "    return np.absolute(result[:,0]), result[:,1] \n",
    "\n",
    "selector = SelectKBest(udf_pearsonr, k=2) # k => number of top features to select\n",
    "selector.fit(train_set, train_y) # fit on trainset\n",
    "transformed_train = selector.transform(train_set) # transform trainset\n",
    "transformed_train.shape #(15000, 2), select the 1st and 7th features\n",
    "assert np.array_equal(transformed_train, train_set[:,[0,6]]) \n",
    "\n",
    "transformed_test = selector.transform(test_set)\n",
    "assert np.array_equal(transformed_test, test_set[:,[0,6]]);\n",
    "# the 1st and 7th features are selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:13.942937Z",
     "start_time": "2020-03-15T22:23:13.935239Z"
    }
   },
   "outputs": [],
   "source": [
    "# validate the result\n",
    "for idx in range(train_set.shape[1]):\n",
    "    pea_score, p_value = pearsonr(train_set[:,idx], train_y)\n",
    "    print(f\"The absolute value of the correlation between the {idx + 1} feature and target is {round(np.abs(pea_score),2)},p-value is {round(p_value,3)}\")\n",
    "# so we should select the 1st and 7th features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Correlation (regression problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance correlation measures the dependence between two continuous features. Unlike the Pearson correlation, distance correlation measures both linear and nonlinear relationships between two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula:  \n",
    "  \n",
    "Firstly, compute the (n x n) distance matrices dX with $dX_{ij}$ as elements and dY with $dY_{ij}$ as elementns containing all pairwise distances. $dX_{ij}$ is the distance between observation i and observation j:    \n",
    "  \n",
    "$dX_{ij} = \\left \\| X_i - X_j \\right \\|$  \n",
    "$dY_{ij} = \\left \\| Y_i - Y_j \\right \\|$\n",
    "  \n",
    "Secondly, we calculate the doubly centered distances as below and update the distance matrices. $\\bar{X_i}$ is the i-th row mean of distance matrices dX, $\\bar{X_j}$ the j-th column mean of distance matrices dX, $\\sum_i^N \\sum_j^N dX_{ij}$ is the grand mean.\n",
    "  \n",
    "$dX_{ij} = dX_{ij} - \\bar{X_i} - \\bar{X_j} + \\frac{1}{N^2} \\sum_i^N \\sum_j^N dX_{ij}$  \n",
    "$dY_{ij} = dY_{ij} - \\bar{Y_i} - \\bar{Y_j} + \\frac{1}{N^2} \\sum_i^N \\sum_j^N dY_{ij}$   \n",
    "  \n",
    "Then, we compute the sample distance covariance/ variance as below:  \n",
    "  \n",
    "$dCov^2 (X, Y) = \\frac{1}{N^2} \\sum_{i}^{N} \\sum_{j}^{N} dX_{ij}dY_{ij}$  \n",
    "$dVar^2(X) = Cov^2_D (X, X)$  \n",
    "\n",
    "Finally, the distance correlation $dCor(X,Y)$ is as below:  \n",
    "  \n",
    "$dCor(X,Y) = \\frac{dCov_D(X, Y)}{\\sqrt{dVar^2(X)}\\sqrt{dVar^2(Y)} }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.572701Z",
     "start_time": "2020-03-15T22:23:13.945012Z"
    }
   },
   "outputs": [],
   "source": [
    "from dcor import distance_correlation\n",
    "from dcor.independence import distance_covariance_test\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# load dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "dataset = fetch_california_housing()\n",
    "X, y = dataset.data, dataset.target # use california_housing dataset as example\n",
    "# in this dataset, both X and y are continuous, so we can use distance Correlation to select features\n",
    "\n",
    "# use the first 15000 obs as train_set\n",
    "# the rest obs as test_set\n",
    "train_set = X[0:15000,:]\n",
    "test_set = X[15000:,]\n",
    "train_y = y[0:15000]\n",
    "\n",
    "# there is no pre-built Distance Correlation function in sklearn\n",
    "# we need to use dcor.distance_correlation (can only compute the distance_correlation between two features) \n",
    "# to create a multivariate version of distance_correlation function that can be used in SelectKBest\n",
    "\n",
    "def udf_dcorr(X, y):\n",
    "    result = np.array([[distance_correlation(x, y), \n",
    "                        distance_covariance_test(x,y)[0]]for x in X.T]) # list of (pearsonr, p-value)\n",
    "    return result[:,0], result[:,1]\n",
    "\n",
    "selector = SelectKBest(udf_dcorr, k=2) # k => number of top features to select\n",
    "selector.fit(train_set, train_y) # fit on trainset\n",
    "transformed_train = selector.transform(train_set) # transform trainset \n",
    "transformed_train.shape #(15000, 2), select the 1st and 3rd features\n",
    "assert np.array_equal(transformed_train, train_set[:,[0,2]])\n",
    "\n",
    "transformed_test = selector.transform(test_set)\n",
    "assert np.array_equal(transformed_test, test_set[:,[0,2]]);\n",
    "# the 1st and 3rd features are selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.583594Z",
     "start_time": "2020-03-15T22:23:12.461Z"
    }
   },
   "outputs": [],
   "source": [
    "# validate the result\n",
    "for idx in range(train_set.shape[1]):\n",
    "    d_score = distance_correlation(train_set[:,idx], train_y)\n",
    "    p_value = distance_covariance_test(train_set[:,idx], train_y)[0]\n",
    "    print(f\"The d-correlation between the {idx + 1} feature and target is {round(d_score,2)}, p-value is {round(p_value,3)}\")\n",
    "# so we should select the 1st and 3rd features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F-Score (regression problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F-Score reports whether any of the independent variables in a linear regression model are significant. Specifically, suppose we have p features, we construct p univariate linear regression for each feature separately, each regress the target variable with the i-th feature and a constant only. Then we can report the F-Score of each linear model which captures the linear relationship between the ith feature and the target variable. The null hypothesis for F-Score is that the feature is not related to the target variable. So we should select features that have higher F-Score (more likely to reject the null hypothesis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula:  \n",
    "\n",
    "$F = \\frac{(SST - SSR)/(p - 1)}{SSR/(n - p)} =  \\frac{SST - SSR}{SSR/(n - 2)} =  \\frac{R^2}{(1 - R^2)(n - 2)} = \\frac{\\rho ^2}{(1 - \\rho ^2)(n - 2)}$  \n",
    "  \n",
    "where:  \n",
    "\n",
    "$SST = \\sum_{i=1}^{n}(y_i - \\overline{y}) ^2$  \n",
    "  \n",
    "$\\overline{y} = \\frac{1}{n} \\sum_{i=1}^{n}y_i$  \n",
    "  \n",
    "$SSR = \\sum_{i=1}^{n}(\\widehat{y}_i - \\overline{y})^2$  \n",
    "  \n",
    "$\\widehat{y}_i$ is the predicted value by the model\n",
    "  \n",
    "SST is the total sum of squares, SSR is the residual sum of squares, p is the number of predictors (including the constant, so p = 2 in our case), $\\rho$ is the correlation coefficient between feature i and the target variable and n is the number of observations. Since in the linear model, there is only one non-constant variable, so $\\rho^2 = R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.584902Z",
     "start_time": "2020-03-15T22:23:12.468Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# load dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "dataset = fetch_california_housing()\n",
    "X, y = dataset.data, dataset.target # use california_housing dataset as example\n",
    "# in this dataset, both X and y are continuous, so we can use F-score to select features\n",
    "\n",
    "# use the first 15000 obs as train_set\n",
    "# the rest obs as test_set\n",
    "train_set = X[0:15000,:]\n",
    "test_set = X[15000:,]\n",
    "train_y = y[0:15000]\n",
    "\n",
    "# use pre-built f-score function in sklearn\n",
    "\n",
    "selector = SelectKBest(f_regression, k=2) # k => number of top features to select\n",
    "selector.fit(train_set, train_y) # fit on trainset \n",
    "transformed_train = selector.transform(train_set) # transform trainset\n",
    "transformed_train.shape #(15000, 2), select the 1st and 7th features\n",
    "assert np.array_equal(transformed_train, train_set[:,[0,6]]) \n",
    "\n",
    "transformed_test = selector.transform(test_set)\n",
    "assert np.array_equal(transformed_test, test_set[:,[0,6]]);\n",
    "# the 1st and 7th features are selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.585850Z",
     "start_time": "2020-03-15T22:23:12.475Z"
    }
   },
   "outputs": [],
   "source": [
    "# validate the result\n",
    "for idx in range(train_set.shape[1]):\n",
    "    score, p_value = f_regression(train_set[:,idx].reshape(-1,1), train_y)\n",
    "    print(f\"The F-Score between the {idx + 1} feature and target is {round(score[0],2)}, p-value is {round(p_value[0],3)}\")\n",
    "# so we should select the 1st and 7th features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutual Information (regression problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual information measures the dependency between the two variables, that is, the reduction in entropy after knowing the information of another variable.   \n",
    "\n",
    "MI is equal to zero if and only if two random variables are independent, and higher values reflect higher dependency. Compared with Pearson correlation & F-Score, it also captures non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula:  \n",
    "  \n",
    "- For discrete distributions (for both x and y):  \n",
    "  \n",
    "    $I(x, y) = H(Y) - H(Y|X) = \\sum_{x\\in \\mathit{X}}  \\sum_{x\\in \\mathit{Y}} \\textit{p}_{(X,Y)}(x,y) \\textrm{log}(\\frac{\\textit{p}_{(X,Y)}(x,y)}{\\textit{p}_{X}(x)\\textit{p}_{Y}(y)})$  \n",
    "\n",
    "    Where $\\textit{p}_{(X,Y)}(x,y)$ is the joint probability mass function (PMF) of x and y, $\\textit{p}_{X}(x)$ is the PMF of x.  \n",
    "  \n",
    "- For continuous distributions (for both x and y):  \n",
    "\n",
    "    $I(X, Y) = H(Y) - H(Y|X) = \\int_X \\int_Y  \\textit{p}_{(X,Y)}(x,y) \\textrm{log}(\\frac{\\textit{p}_{(X,Y)}(x,y)}{\\textit{p}_{X}(x)\\textit{p}_{Y}(y)}) \\, \\, dx dy$  \n",
    "    \n",
    "    Where $\\textit{p}_{(X,Y)}(x,y)$ is the joint probability density function (PDF) of x and y, $\\textit{p}_{X}(x)$ is the PDF of x.  \n",
    "  \n",
    "  \n",
    "But in reality, is it likely that one of x and y is discrete variable and the another is continuous variable. So in sklearn, it implement the nonparametric methods based on entropy estimation from k-nearest neighbors distances proposed in [1] and [2].  \n",
    "  \n",
    "  \n",
    "[1] A. Kraskov, H. Stogbauer and P. Grassberger, âEstimating mutual informationâ. Phys. Rev. E 69, 2004.  \n",
    "[2] B. C. Ross âMutual Information between Discrete and Continuous Data Setsâ. PLoS ONE 9(2), 2014. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.586825Z",
     "start_time": "2020-03-15T22:23:12.482Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# load dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "dataset = fetch_california_housing()\n",
    "X, y = dataset.data, dataset.target # use california_housing dataset as example\n",
    "# in this dataset, both X and y are continuous, so we can use MI to select features\n",
    "\n",
    "# use the first 15000 obs as train_set\n",
    "# the rest obs as test_set\n",
    "train_set = X[0:15000,:].astype(float)\n",
    "test_set = X[15000:,].astype(float)\n",
    "train_y = y[0:15000].astype(float)\n",
    "\n",
    "# since n_neighbors in the KNN is also a very important parameters\n",
    "# so we write a new MI function function based on pre-built MI function in sklearn\n",
    "# to allow more flexibility\n",
    "\n",
    "def udf_MI(X, y):\n",
    "    result = mutual_info_regression(X, y, n_neighbors = 5) # user_defined n_neighbors\n",
    "    return result\n",
    "\n",
    "selector = SelectKBest(udf_MI, k=2) # k => number of top features to select\n",
    "selector.fit(train_set, train_y) # fit on trainset\n",
    "transformed_train = selector.transform(train_set) # transform trainset\n",
    "transformed_train.shape #(15000, 2), select the 1st and 8th features\n",
    "assert np.array_equal(transformed_train, train_set[:,[0,7]]) # return True\n",
    "\n",
    "transformed_test = selector.transform(test_set) # transform test set\n",
    "assert np.array_equal(transformed_test, test_set[:,[0,7]]);\n",
    "# the 1st and 8th features are selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.587843Z",
     "start_time": "2020-03-15T22:23:12.487Z"
    }
   },
   "outputs": [],
   "source": [
    "# validate the result\n",
    "for idx in range(train_set.shape[1]):\n",
    "    score = mutual_info_regression(train_set[:,idx].reshape(-1,1), train_y, n_neighbors = 5)\n",
    "    print(f\"The MI between the {idx + 1} feature and target is {round(score[0],2)}\")\n",
    "# so we should select the 1st and 8th features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-squared Statistics (classification problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chi-Square Statistic determines whether there is a relationship between categorical variables. Sklearn provides chi2 function to calculate Chi-square. The input of this function should be booleans or frequencies. The null hypothesis is that two variables are independent, so the higher the chi-square, the higher the probability that the two variables are correlated, so we should select features that have higher chi-square scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula:  \n",
    "  \n",
    "$\\chi^2 = \\sum_{i=1}^{r} \\sum_{j=1}^{c} \\frac{(O_{i,j} - E_{i,j})^2}{E_{i,j}} = n \\sum_{i,j} p_ip_j(\\frac{\\frac{O_{i,j}}{n} - p_i p_j}{p_i p_j})^2$  \n",
    "  \n",
    "$O_{i,j}$ is the number of observations that have the i-th category value in feature X and\n",
    "the j-th category value in feature Y. $E_{i,j}$ is the expected number of observations that have the i-th category value in feature X and\n",
    "the j-th category value in feature Y. n is the number of observations in the dataset. $p_i$ is the probability of having the i-th category value in feature X, $p_j $ is the probability of having the j-th category value in feature Y.  \n",
    " \n",
    "**We did some research on the original source code of sklearn. And we figure out that in fact, the chi-square statistic calculated using chi2 function in sklearn is not the real Chi-square statistic**. When the input feature is a boolean feature, the value calculated by chi2 function only considers the situation when the feature value is True (we will elaborate this later). This has the advantage that the sum of the output calculated by the chi2 equation for all the boolean variables generated by one-hot encoding will equal to the chi-square statistics of the original variable.  \n",
    "  \n",
    "For example, suppose a variable I has three possible categorical values: 0, 1, and 2. After the one-hot encoding, three new boolean variables will be generated. The sum of the chi2 function output of these three boolean variables equals the chi-square statistics calculated directly between the original variable I and the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain how sklearn calculate chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.588826Z",
     "start_time": "2020-03-15T22:23:12.495Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate a dataset\n",
    "import pandas as pd\n",
    "sample_dict = {'Type': ['J','J','J',\n",
    "                        'B','B','B',\n",
    "                        'C','C','C','C','C'], \n",
    "               'Output': [0, 1, 0, \n",
    "                          2, 0, 1,  \n",
    "                          0, 0, 1, 2, 2,]}\n",
    "sample_raw = pd.DataFrame(sample_dict)\n",
    "sample_raw # raw data, `output` is our target variable, `Type` is the input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.589740Z",
     "start_time": "2020-03-15T22:23:12.500Z"
    }
   },
   "outputs": [],
   "source": [
    "# one-hot encoding the dataset and generate boolean variable\n",
    "# use sklearn chi2 to calculate chi2 value for each boolean variables\n",
    "\n",
    "sample = pd.get_dummies(sample_raw)\n",
    "from sklearn.feature_selection import chi2\n",
    "chi2(sample.values[:,[1,2,3]],sample.values[:,[0]])\n",
    "# the first row is the chi2 value for each boolean variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.591472Z",
     "start_time": "2020-03-15T22:23:12.506Z"
    }
   },
   "outputs": [],
   "source": [
    "# now calculate the chi2 between original feature `Type` and target variable `Output`\n",
    "# calculate the contingency table first\n",
    "obs_df = sample_raw.groupby(['Type','Output']).size().reset_index()\n",
    "obs_df.columns = ['Type','Output','Count']\n",
    "obs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the contingency table is as below:\n",
    "\n",
    "| Type/Output | 0 | 1 | 2 |\n",
    "|------|------|------|------|\n",
    "|  B  | 1 | 1 | 1 |\n",
    "|  C  | 2 | 1 | 2 |\n",
    "|  J  | 2 | 1 | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.593466Z",
     "start_time": "2020-03-15T22:23:12.512Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "obs = np.array([[1, 1, 1], [2, 1, 2],[2, 1, 0]])\n",
    "chi2_contingency(obs) # the first value is the chi2 stats between the orginal feature\n",
    "# and the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.594645Z",
     "start_time": "2020-03-15T22:23:12.516Z"
    }
   },
   "outputs": [],
   "source": [
    "# the sum of the sklearn.chi2 results equal to the chi-square stats between the orginal feature\n",
    "# and the target variable\n",
    "\n",
    "chi2(sample.values[:,[1,2,3]],sample.values[:,[0]])[0].sum() == chi2_contingency(obs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.596249Z",
     "start_time": "2020-03-15T22:23:12.521Z"
    }
   },
   "outputs": [],
   "source": [
    "# then how the chi2 is calcualted in sklearn?\n",
    "# take the first result 0.17777778 as an example\n",
    "# this is the chi2 value sklearn calculated for Type B boolean variable\n",
    "# this value 0.17777778 is the same as the result from the below code\n",
    "\n",
    "from scipy.stats import chisquare\n",
    "f_exp = np.array([5/11, 3/11, 3/11]) * 3 # expected occurance = the prior prob of output variable * the numebr of obs in B Type\n",
    "chisquare([1,1,1], f_exp=f_exp) # [1,1,1] is the actual occurance of obsin B Type\n",
    "\n",
    "# so the chi2 function in sklearn only considers the obs in B Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use sklearn chi2 function for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.597531Z",
     "start_time": "2020-03-15T22:23:12.528Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# load the dataset\n",
    "from sklearn.datasets import load_iris # use iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "# in this dataset, X is continuous variable, y is categorical variable\n",
    "# does not meet the chi2 requirement \n",
    "\n",
    "# convert to categorical data by converting data to booleans\n",
    "# as demo, convert continuous variable to booleans by whether the value is greater than mean\n",
    "X = X > X.mean(0)\n",
    "\n",
    "# before using iris dataset, we need to shuffle the dataset first\n",
    "np.random.seed(1234)\n",
    "idx = np.random.permutation(len(X))\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "# use the first 100 obs as train_set\n",
    "# the rest 50 obs as test_set\n",
    "train_set = X[0:100,:]\n",
    "test_set = X[100:,]\n",
    "train_y = y[0:100]\n",
    "\n",
    "# use pre-built chi-squared function in sklearn\n",
    "selector = SelectKBest(chi2, k=2) # # k => number of top features to select\n",
    "selector.fit(train_set, train_y) # fit on trainset \n",
    "transformed_train = selector.transform(train_set) # transform trainset\n",
    "transformed_train.shape #(100, 2), select the 3rd and 4th features\n",
    "assert np.array_equal(transformed_train, train_set[:,[2,3]]) \n",
    "\n",
    "transformed_test = selector.transform(test_set) # transform test set\n",
    "assert np.array_equal(transformed_test, test_set[:,[2,3]]);\n",
    "# select the 3rd and 4th features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.598650Z",
     "start_time": "2020-03-15T22:23:12.534Z"
    }
   },
   "outputs": [],
   "source": [
    "# validate the result\n",
    "for idx in range(train_set.shape[1]):\n",
    "    score, p_value = chi2(train_set[:,idx].reshape(-1,1), train_y)\n",
    "    print(f\"The ch2 statistics between the {idx + 1} feature and target is {round(score[0],2)}, p-value is {round(p_value[0],3)}\")\n",
    "# so we should select the 3rd and 4th features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F-Score (classification problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification tasks, if the features are categorical, then we can use the chi-square statistic to select the top features. However, if features are continuous, then we should use ANOVA F-value. The ANOVA F-value scores examine if we group the numerical feature by the target variable (category), the population means for each group are significantly different. The null hypothesis is that these mean are the same. So we should select features output higher F-Score since higher F-Score means that we should reject the hypothesis therefore features with higher F-Score are more related to the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula:  \n",
    "  \n",
    "$F = \\frac{MSB}{MSE} = \\frac{ \\frac{SS(between)}{m-1}}{ \\frac{SS(error)}{n-m}}$  \n",
    "  \n",
    "Where SS(between) is the Sum of Squares between the groups, specifically the sum of squares between the group means and the grand mean. SS(error) is the Sum of Squares within the groups, specifically sum of squares between the data and the group means. m is the number of groups compared, n is the number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.600144Z",
     "start_time": "2020-03-15T22:23:12.540Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.datasets import load_iris # use iris dataset as example\n",
    "\n",
    "# use iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "# in this dataset, X is continuous variable, y is categorical, so we can use \n",
    "# ANOVA-F score to select features\n",
    "\n",
    "# before using iris dataset, we need to shuffle the dataset first\n",
    "np.random.seed(1234)\n",
    "idx = np.random.permutation(len(X))\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "# use the first 100 obs as train_set\n",
    "# the rest 50 obs as test_set\n",
    "train_set = X[0:100,:]\n",
    "test_set = X[100:,]\n",
    "train_y = y[0:100]\n",
    "\n",
    "# use pre-built f-score function in sklearn\n",
    "selector = SelectKBest(f_classif, k=2) # k => number of top features to select\n",
    "selector.fit(train_set, train_y) # fit on train set\n",
    "transformed_train = selector.transform(train_set) # transform train set\n",
    "transformed_train.shape #(100, 2), select the 3rd and 4th features\n",
    "assert np.array_equal(transformed_train, train_set[:,[2,3]]) # return True\n",
    "\n",
    "transformed_test = selector.transform(test_set) # transform test set\n",
    "assert np.array_equal(transformed_test, test_set[:,[2,3]]);\n",
    "# select the 3rd and 4th features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.601484Z",
     "start_time": "2020-03-15T22:23:12.547Z"
    }
   },
   "outputs": [],
   "source": [
    "# validate the result\n",
    "for idx in range(train_set.shape[1]):\n",
    "    score, p_value = f_classif(train_set[:,idx].reshape(-1,1), train_y)\n",
    "    print(f\"The ANOVA F-Score between the {idx + 1} feature and target is {round(score[0],2)}, p-value is {round(p_value[0],3)}\")\n",
    "# so we should select the 3rd and 4th features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutual Information (classification problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual information measures the dependency between the two variables. MI is equal to zero if and only if two random variables are independent, and higher values reflect higher dependency. Compared with Pearson correlation & F-Score, it also captures the non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula (the same as 1.1.1.5):  \n",
    "  \n",
    "- For discrete distributions (for both x and y):  \n",
    "  \n",
    "    $I(x, y) = \\sum_{x\\in \\mathit{X}}  \\sum_{x\\in \\mathit{Y}} \\textit{p}_{(X,Y)}(x,y) \\textrm{log}(\\frac{\\textit{p}_{(X,Y)}(x,y)}{\\textit{p}_{X}(x)\\textit{p}_{Y}(y)})$  \n",
    "\n",
    "    Where $\\textit{p}_{(X,Y)}(x,y)$ is the joint probability mass function (PMF) of x and y, $\\textit{p}_{X}(x)$ is the PMF of x.  \n",
    "  \n",
    "- For continuous distributions (for both x and y):  \n",
    "\n",
    "    $I(X, Y) = \\int_X \\int_Y  \\textit{p}_{(X,Y)}(x,y) \\textrm{log}(\\frac{\\textit{p}_{(X,Y)}(x,y)}{\\textit{p}_{X}(x)\\textit{p}_{Y}(y)}) \\, \\, dx dy$  \n",
    "    \n",
    "    Where $\\textit{p}_{(X,Y)}(x,y)$ is the joint probability density function (PDF) of x and y, $\\textit{p}_{X}(x)$ is the PDF of x.  \n",
    "  \n",
    "  \n",
    "But in reality, is it likely that one of x and y is discrete variable and the another is continuous variable. So in sklearn, it implement the nonparametric methods based on entropy estimation from k-nearest neighbors distances proposed in [1] and [2].  \n",
    "  \n",
    "  \n",
    "[1] A. Kraskov, H. Stogbauer and P. Grassberger, âEstimating mutual informationâ. Phys. Rev. E 69, 2004.  \n",
    "[2] B. C. Ross âMutual Information between Discrete and Continuous Data Setsâ. PLoS ONE 9(2), 2014. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.602616Z",
     "start_time": "2020-03-15T22:23:12.553Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.datasets import load_iris # use iris dataset as example\n",
    "\n",
    "# use iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "# in this dataset, X is continuous variable, y is categorical, so we can use \n",
    "# MI to select features\n",
    "\n",
    "# before using iris dataset, we need to shuffle the dataset first\n",
    "np.random.seed(1234)\n",
    "idx = np.random.permutation(len(X))\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "# use the first 100 samples as train_set\n",
    "# the rest 5 samples as test_set\n",
    "train_set = X[0:100,:].astype(float)\n",
    "test_set = X[100:,].astype(float)\n",
    "train_y = y[0:100].astype(float)\n",
    "\n",
    "# since n_neighbors in the KNN is also a very important parameters\n",
    "# so we write a new MI function function based on pre-built MI function in sklearn\n",
    "# to allow more flexibility\n",
    "\n",
    "def udf_MI(X, y):\n",
    "    result = mutual_info_classif(X, y, n_neighbors = 5) # user_defined n_neighbors\n",
    "    return result\n",
    "\n",
    "selector = SelectKBest(f_classif, k=2) # k => number of top features to select\n",
    "selector.fit(train_set, train_y) # fit on the trainset\n",
    "transformed_train = selector.transform(train_set) # transform test set\n",
    "transformed_train.shape #(100, 2), select the 3rd and 4th features\n",
    "assert np.array_equal(transformed_train, train_set[:,[2,3]]) # return True\n",
    "\n",
    "transformed_test = selector.transform(test_set) # transform test set\n",
    "assert np.array_equal(transformed_test, test_set[:,[2,3]]);\n",
    "# so we should select the 3rd and 4th features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.603675Z",
     "start_time": "2020-03-15T22:23:12.558Z"
    }
   },
   "outputs": [],
   "source": [
    "# validate the result\n",
    "for idx in range(train_set.shape[1]):\n",
    "    score = mutual_info_classif(train_set[:,idx].reshape(-1,1), train_y, n_neighbors = 5)\n",
    "    print(f\"The MI between the {idx + 1} feature and target is {round(score[0],2)}\")\n",
    "# so we should select the 3rd and 4th features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Filter Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with Univariate Filter Methods, Multivariate filter methods select the best feature based on the entire feature space. The relationships between features are taken into account so it performs better in removing redundant features. Here utilize the [skfeature](https://github.com/jundongl/scikit-feature) module developed by Arizona State University."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max-Relevance Min-Redundancy (mRMR) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mRMR method tries to find a subset of features that have a higher association (MI) with the target variable while at the same time have lower inter-association with all the features already in the subset. The implementation of mRMR in skfeature only works for discrete features in the classification problems since it uses discrete the Shannon information term during the calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula:  \n",
    "\n",
    "Assuming dataset contains m features, the n-th feature importance based on the mRMR criterion for feature $X_i$ can be expressed as:  \n",
    "  \n",
    "$f^{mRMR}(X_i) = I(Y, X_i) - \\frac{1}{|S|}\\sum_{X_s \\in S} I(X_s, X_i)$  \n",
    "\n",
    "$I(Y, X_i)$ is the MI between feature $X_i$ and target variable. $\\frac{1}{|S|}\\sum_{X_s \\in S} I(X_s, X_i)$ is the average MI between feature $X_i$ and all the features already in the subset.  \n",
    "\n",
    "At each step of the mRMR feature selection process, the feature $X_i, (X_i \\notin  S)$ with the highest feature importance score $f^{mRMR}(X_i)$ will be added to the subset until reach desired number of featuresin the subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.604993Z",
     "start_time": "2020-03-15T22:23:12.566Z"
    }
   },
   "outputs": [],
   "source": [
    "from skfeature.function.information_theoretical_based import MRMR\n",
    "from sklearn.datasets import load_iris # use iris dataset as example\n",
    "\n",
    "# use iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# use the first 100 samples as train_set\n",
    "# the rest 5 samples as test_set\n",
    "# since the mRMR in skfeature only works for distrete features\n",
    "# so we cast float to int and make continuous variables into distrete for example only.\n",
    "train_set = X[0:100,:].astype(int)\n",
    "test_set = X[100:,].astype(int)\n",
    "train_y = y[0:100].astype(int)\n",
    "\n",
    "feature_index,_,_ = MRMR.mrmr(train_set, train_y, n_selected_features=2)\n",
    "transformed_train = train_set[:,feature_index] # select the 3rd and 4th features\n",
    "transformed_test = test_set[:,feature_index] # select the 3rd and 4th features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation-based Feature Selection (CFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CFS method is based on a simple assumption: Good feature subsets should contain features highly correlated with the target and uncorrelated to each other. The implementation of CFS in skfeature only works for discrete features in the classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula:  \n",
    "\n",
    "$CFS = \\underset{S_k}{max} [\\frac{ \\sum_{i=1}^{k} r_{f_kc}}{\\sqrt{k +2 ( \\sum_{i=1}^{k} \\sum_{j=1}^{k} r_{f_if_j}) }}]$  \n",
    "\n",
    "$S_k$ is the subset. $r_{f_kc}$ is the correlation between target variable c and feature $X_k, X_k \\in S_k$. $r_{f_if_j}$ is the correlation between variable $X_i, X_i \\in S_k$ and variable $X_j, X_j \\in S_k$.  \n",
    "\n",
    "In the skfeature package, the correlation measure is based on symmetrical uncertainty (SU), which works in two discrete variables only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.605991Z",
     "start_time": "2020-03-15T22:23:12.576Z"
    }
   },
   "outputs": [],
   "source": [
    "from skfeature.function.statistical_based import CFS\n",
    "from sklearn.datasets import load_iris # use iris dataset as example\n",
    "\n",
    "# use iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# use the first 100 samples as train_set\n",
    "# the rest 5 samples as test_set\n",
    "# since the mRMR in skfeature only works for distrete features\n",
    "# so we cast float to int and make continuous variables into distrete for example only.\n",
    "train_set = X[0:100,:].astype(int)\n",
    "test_set = X[100:,].astype(int)\n",
    "train_y = y[0:100].astype(int)\n",
    "\n",
    "num_feature = 2 # we select a subset contains two features\n",
    "feature_index = CFS.cfs(train_set, train_y)\n",
    "transformed_train = train_set[:,feature_index[0:num_feature]] # select the 3rd and 4th features\n",
    "transformed_test = test_set[:,feature_index[0:num_feature]]  #select the 3rd and 4th features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T21:15:44.699047Z",
     "start_time": "2020-01-16T21:15:44.694402Z"
    }
   },
   "source": [
    "#### Fast Correlation-based Filter (FCBF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FCBF works faster and more efficiently than CFS. It has a procedure very similar to the mRMR, but it uses symmetrical uncertainty (SU) as the goodness measure. It sorts features by their SU value with the target variable from the highest to the lowest. Then it removes redundant features that have higher SU value with features already in the candidate list than their SU value with the target variable. Similar to CFS, the FCBF implemented in skfeature only works in classification problems with discrete variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.607430Z",
     "start_time": "2020-03-15T22:23:12.584Z"
    }
   },
   "outputs": [],
   "source": [
    "from skfeature.function.information_theoretical_based import FCBF\n",
    "from sklearn.datasets import load_iris # use iris dataset as example\n",
    "\n",
    "# use iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# use the first 100 samples as train_set\n",
    "# the rest 5 samples as test_set\n",
    "# since the mRMR in skfeature only works for distrete features\n",
    "# so we cast float to int and make continuous variables into distrete for example only.\n",
    "train_set = X[0:100,:].astype(int)\n",
    "test_set = X[100:,].astype(int)\n",
    "train_y = y[0:100].astype(int)\n",
    "\n",
    "num_feature = 2 # we select a subset contains two features\n",
    "feature_index = FCBF.fcbf(train_set, train_y, n_selected_features=2)[0] # select two features only\n",
    "transformed_train = train_set[:,feature_index[0:num_feature]] \n",
    "# only the 4th feature is selected!\n",
    "transformed_test = test_set[:,feature_index[0:num_feature]]  # only the 4th feature is selected!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReliefF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relief F method is based on the Relief method. The Relief method is a Feature weighting algorithm. It assigns features with higher weights if they have a higher correlation with the target variable (binary classification), and removes features with weights below a certain threshold. The correlation in the Relief method is defined as the ability to discriminate close samples.\n",
    "\n",
    "In each step, the algorithm randomly selects a sample S from the training set, and then it finds the nearest neighbor sample of S that has the same target label, called it NearHit. It will also find the nearest neighbor sample of S that has a different label, called it NearMiss. It then updates the weight of each feature according to the following rules:\n",
    "\n",
    "1) If the distance between sample S and Near Hit on a feature is less than the distance between R and Near Miss, the weight of the feature will be increasedÂ since the feature is beneficial for discriminating between the nearest neighbors of the same kind and different labels.\n",
    "2) Conversely, if the distance between R and Near Hit on a feature is greater than the distance on R and Near Miss, the weight of the feature is reduced.\n",
    "\n",
    "The above process is repeated over m times, and finally, the average weight of each feature is obtained. The larger the weight of a feature, the stronger the classification ability of the feature.\n",
    "\n",
    "In Relief F, it modified the way to update the weights so it can be applied to the multi-class classification problem. Also, it randomly samples K nearest samples instead of one. Â \n",
    "Â  \n",
    "ReliefF implemented in skfeature works for continuous features or binary categorical features in classification problems since it uses L1 norm to measure the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula:  \n",
    "  \n",
    "$W(A) = W(A) - \\frac{\\sum_{i=1}^{k} diff(A, S, H_j) }{mk} + \\frac{\\sum_{C\\notin class(S)} [\\frac{p(C)}{1-P(class(S))} \\sum_{i=1}^{k} diff(A, S, M_j(C)) ]}{mk}$  \n",
    "\n",
    "$diff(A, R_1, R_2) = \\left\\{\\begin{matrix}\n",
    "\\frac{|R_1(A) - R_2(A)|}{max(A)- min(A)} & if A\\ is\\ continuous\\\\ \n",
    " 0 & if A\\ is\\ discrete\\ and \\ R_1(A) =R_2(A)\\\\ \n",
    " 1 & if A\\ is\\ discrete\\ and \\ R_1(A) \\neq R_2(A)\n",
    "\\end{matrix}\\right.$  \n",
    "  \n",
    "$R_1$ and $R_2$ are two samples. A is the feature we are working on. S is the sample we selected. $H_j$ is the j-th NearHit, $M_j$ is the j-th NearMiss. C are the classes that are different from the class of sample we selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.608799Z",
     "start_time": "2020-03-15T22:23:12.594Z"
    }
   },
   "outputs": [],
   "source": [
    "from skfeature.function.similarity_based import reliefF\n",
    "from sklearn.datasets import load_iris # use iris dataset as example\n",
    "\n",
    "# use iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# use the first 100 samples as train_set\n",
    "# the rest 5 samples as test_set\n",
    "# all the features are continuous variable\n",
    "train_set = X[0:100,:]\n",
    "test_set = X[100:,]\n",
    "train_y = y[0:100]\n",
    "\n",
    "num_feature = 2 # we select a subset contains two features\n",
    "score = reliefF.reliefF(train_set, train_y)\n",
    "feature_index = reliefF.feature_ranking(score) # ranked index\n",
    "transformed_train = train_set[:,feature_index[0:num_feature]] # select the 3rd and 4th features\n",
    "transformed_test = test_set[:,feature_index[0:num_feature]]  # select the 3rd and 4th features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T21:48:57.048462Z",
     "start_time": "2020-01-16T21:48:57.043984Z"
    }
   },
   "source": [
    "#### Spectral Feature Selection (SPEC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPEC method is an unsupervised method built on spectral graph theory. It firstly build up the similarity set S and constructing its graph representation. Then it evaluates features based on the spectrum of the constructed graph. The SPEC implemented in skfeature works for continuous features or binary categorical features in classification problems since it uses the RBF (Gaussian) kernel as the similarity set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.610382Z",
     "start_time": "2020-03-15T22:23:12.600Z"
    }
   },
   "outputs": [],
   "source": [
    "from skfeature.function.similarity_based import SPEC\n",
    "from sklearn.datasets import load_iris # use iris dataset as example\n",
    "\n",
    "# use iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# use the first 100 samples as train_set\n",
    "# the rest 5 samples as test_set\n",
    "# all the features are continuous variable\n",
    "train_set = X[0:100,:]\n",
    "test_set = X[100:,]\n",
    "train_y = y[0:100]\n",
    "\n",
    "num_feature = 2 # we select a subset contains two features\n",
    "score = SPEC.spec(train_set)\n",
    "feature_index = SPEC.feature_ranking(score) \n",
    "transformed_train = train_set[:,feature_index[0:num_feature]]  # select the 1st and 2nd features\n",
    "transformed_test = test_set[:,feature_index[0:num_feature]]  # select the 1st and 2nd features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper methods take the feature selection problem as a search problem and it evaluates combinations of features by a predictive model. Every time it trains a model on a subset of features, then evaluated it, and keeps on adjusting the subsets and model until it finds the optimal subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T03:17:41.041775Z",
     "start_time": "2020-02-01T03:17:40.925277Z"
    }
   },
   "source": [
    "![image](./images/Wrapper_Pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deterministic Algorithms will always output the same subset of features given the same data input. Sequential Forward Selection (SFS), Sequential Backward Selection (SBS) are examples of the Deterministic Algorithm.\n",
    "\n",
    "SFS starts from a model fitting on one single feature, and every step, it keeps on adding one new feature into the existing model that outputs the best performance until the number of selected features meets theÂ requirement.\n",
    "\n",
    "Whereas, SBS starts from a model fitting on all features, and every step, it keeps on deleting the lease important feature from the subset that until the number of selected features meets theÂ requirement.\n",
    "\n",
    "But both SFS & SBS are likely to get stuck at local optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive Feature Elimination (SBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklearn, it implements Recursive Feature Elimination (SBS) only. Sklearn provides two Recursive Feature Elimination functions, one is RFE and the other is RFECV. Compared with RFE function, theÂ REFCV function uses cross-validated results to select the best number of features, whereas in RFE, the number of features to select is predefined by users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.612380Z",
     "start_time": "2020-03-15T22:23:12.621Z"
    }
   },
   "outputs": [],
   "source": [
    "# RFE\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# load dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "dataset = fetch_california_housing()\n",
    "X, y = dataset.data, dataset.target # use california_housing dataset as example\n",
    "\n",
    "# use the first 15000 samples as train_set\n",
    "# the rest samples as test_set\n",
    "train_set = X[0:15000,:]\n",
    "test_set = X[15000:,]\n",
    "train_y = y[0:15000]\n",
    "\n",
    "# define a predictive model\n",
    "from sklearn.ensemble import ExtraTreesRegressor # we use extratree\n",
    "clf = ExtraTreesRegressor(n_estimators=25)\n",
    "selector = RFE(estimator = clf, n_features_to_select = 4, step = 1) # no cv in RFE\n",
    "# select 4 features, each step we only remove one feature\n",
    "selector = selector.fit(train_set, train_y)\n",
    "\n",
    "transformed_train = train_set[:,selector.support_]  # select the 1st, 6th, 7th, 8th features\n",
    "transformed_test = test_set[:,selector.support_] # select the 1st, 6th, 7th, 8th features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.613971Z",
     "start_time": "2020-03-15T22:23:12.635Z"
    }
   },
   "outputs": [],
   "source": [
    "# RFECV\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# load dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "dataset = fetch_california_housing()\n",
    "X, y = dataset.data, dataset.target # use california_housing dataset as example\n",
    "\n",
    "# use the first 15000 samples as train_set\n",
    "# the rest samples as test_set\n",
    "train_set = X[0:15000,:]\n",
    "test_set = X[15000:,]\n",
    "train_y = y[0:15000]\n",
    "\n",
    "# define a predictive model\n",
    "from sklearn.ensemble import ExtraTreesRegressor # we use extratree\n",
    "clf = ExtraTreesRegressor(n_estimators=25)\n",
    "selector = RFECV(estimator = clf, step = 1, cv = 5) # use 5-fold cross validation\n",
    "# select 4 features, each step we only remove one feature\n",
    "selector = selector.fit(train_set, train_y)\n",
    "\n",
    "transformed_train = train_set[:,selector.support_]  # select all the features\n",
    "transformed_test = test_set[:,selector.support_] # select all the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T16:49:13.141829Z",
     "start_time": "2020-01-17T16:49:13.137293Z"
    }
   },
   "source": [
    "### Randomized Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with the Deterministic Algorithm, the Randomized Algorithms employs a degree of randomness when searching the best subset. So it might output different subsets of features given the same data input but the randomness will help the model avoid stuck at the local optimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulated Annealing (SA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulated annealing is a controlled random search method. Each time, we will select a feature subset completely at random based on the current solution. If the new subset works better, then we will adopt it. If the new subset works worse, we will still accept it but at some probability determined by the current state.\n",
    "\n",
    "Accepting a worse solution is crucial in the SA algorithm because this helps to avoid stuck at local optimal. As the iteration going, the SA algorithm should reach and converge to a good and stable solution.\n",
    "\n",
    "Since currently there aren't any packages implemented the SA algorithm well, so I wrote a [python script](./SA.py) to implement the SA algorithm for your reference. It supports both classification and regression problems. It also provides cross-validation support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula:  \n",
    "  \n",
    "The probability of accepting a worse solution is as below:  \n",
    "$Prob = exp( - \\frac{loss_{n} - loss_{o}}{k * Cur\\_{Temperature}})$  \n",
    "  \n",
    "Where $loss_n$ is the new loss and $loss_o$ is the lowest score achieved before fitting the new model. \n",
    "  \n",
    "The pseudo code of the SA algorithm:  \n",
    "\n",
    "![image](./images/SA_Pseudo_Code.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.615431Z",
     "start_time": "2020-03-15T22:23:12.648Z"
    }
   },
   "outputs": [],
   "source": [
    "from SA import Simulated_Annealing # import the python script with SA.\n",
    "\n",
    "# regression example\n",
    "# load the dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "dataset = fetch_california_housing()\n",
    "X, y = dataset.data, dataset.target # use california_housing dataset as example\n",
    "\n",
    "# use the first 15000 samples as train_set\n",
    "# the rest samples as test_set\n",
    "train_set = X[0:15000,:]\n",
    "test_set = X[15000:,]\n",
    "train_y = y[0:15000]\n",
    "\n",
    "# define a predictive model\n",
    "from sklearn.ensemble import ExtraTreesRegressor # we use extratree as predictive model\n",
    "\n",
    "# define the loss function in SA algorithm\n",
    "from sklearn.metrics import mean_squared_error # for regression task we use MSE\n",
    "\n",
    "clf = ExtraTreesRegressor(n_estimators=25)\n",
    "selector = Simulated_Annealing(loss_func = mean_squared_error, estimator = clf, \n",
    "                               init_temp = 0.2, min_temp = 0.005, iteration = 10, alpha = 0.9)\n",
    "# parameters detail can be viewed from SA.py\n",
    "\n",
    "selector.fit(X_train = train_set, y_train = train_y, cv = 5) # use 5-fold cross-validation\n",
    "\n",
    "transformed_train = selector.transform(train_set) # trainset after feature selection\n",
    "transformed_test = selector.transform(test_set)  # testset after feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.617021Z",
     "start_time": "2020-03-15T22:23:12.658Z"
    }
   },
   "outputs": [],
   "source": [
    "selector.best_sol # return the best solution feature index;\n",
    "selector.best_loss; # return the loss associated with the best solution; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.619343Z",
     "start_time": "2020-03-15T22:23:12.667Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from SA import Simulated_Annealing # import the python script with SA.\n",
    "\n",
    "# classification example\n",
    "# use iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# random suffle the dataset\n",
    "# use the first 100 samples as train_set\n",
    "# the rest 5 samples as test_set\n",
    "idx = np.random.permutation(len(X))\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "train_set = X[0:100,:]\n",
    "test_set = X[100:,]\n",
    "train_y = y[0:100]\n",
    "test_y = y[100:,]\n",
    "\n",
    "# define a predictive model\n",
    "from sklearn.ensemble import ExtraTreesClassifier # we use extratree as predictive model\n",
    "\n",
    "# define the loss function in SA algorithm\n",
    "from sklearn.metrics import log_loss # for classification we use entropy\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=25)\n",
    "selector = Simulated_Annealing(loss_func = log_loss, estimator = clf, \n",
    "                               init_temp = 0.2, min_temp = 0.005, iteration = 10, \n",
    "                               alpha = 0.9, predict_type = 'predict_proba')\n",
    "# parameters detail can be viewed from SA.py\n",
    "\n",
    "selector.fit(X_train = train_set, y_train = train_y, X_val = test_set, \n",
    "             y_val = test_y, stop_point = 10)\n",
    "\n",
    "transformed_train = selector.transform(train_set)  # trainset after feature selection\n",
    "transformed_test = selector.transform(test_set)  # testset after feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.621009Z",
     "start_time": "2020-03-15T22:23:12.673Z"
    }
   },
   "outputs": [],
   "source": [
    "selector.best_sol # return the best solution feature index;\n",
    "selector.best_loss; # return the loss associated with the best solution; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genetic Algorithm (GA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The genetic algorithm is a stochastic optimization tool based on concepts of evolution population biology. It mimics the evolutionary process in nature and solves the optimization problem by allowing the solutions to reproduce and create new solutions (generations) by \"crossover\" and \"mutation\". It also incorporates the competition concept and only allows the fittest solutions (in our cases, feature subsets that result in the lowest loss) to survive and populate their subsequent generation. After a certain generation evolving, GA should converge to an optimization solution.\n",
    "\n",
    "I also wrote a [python script](GA.py) to implement the GA algorithm for your reference. It provides two algorithms including 'one-max' and 'NSGA2'. 'One-max' is the traditional one objective GA algorithm and 'NSGA2' is a multi-objective GA algorithm. In our case, the target of 'one-max' is to reduce the loss, while the target of 'NSGA2' is to minimize both the loss and also the number of features in the subset. \n",
    "  \n",
    "The python script supports both classification and regression problems. It also provides cross-validation support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudo code of the GA algorithm:  \n",
    "\n",
    "![image](./images/GA_Pseudo_Code.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.622560Z",
     "start_time": "2020-03-15T22:23:12.689Z"
    }
   },
   "outputs": [],
   "source": [
    "from GA import Genetic_Algorithm # import the python script with GA.\n",
    "\n",
    "# regression example\n",
    "# load the dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "dataset = fetch_california_housing()\n",
    "X, y = dataset.data, dataset.target # use california_housing dataset as example\n",
    "\n",
    "# use the first 15000 samples as train_set\n",
    "# the rest samples as test_set\n",
    "train_set = X[0:15000,:]\n",
    "test_set = X[15000:,]\n",
    "train_y = y[0:15000]\n",
    "\n",
    "# define a predictive model\n",
    "from sklearn.ensemble import ExtraTreesRegressor # we use extratree as predictive model\n",
    "\n",
    "# define the loss function in SA algorithm\n",
    "from sklearn.metrics import mean_squared_error # for regression task we use MSE\n",
    "\n",
    "clf = ExtraTreesRegressor(n_estimators=25)\n",
    "selector = Genetic_Algorithm(loss_func = mean_squared_error, estimator = clf, \n",
    "                             n_gen = 10, n_pop = 20, algorithm = 'NSGA2')\n",
    "# parameters detail can be viewed from GA.py\n",
    "\n",
    "selector.fit(X_train = train_set, y_train = train_y, cv = 5) # use 5-fold cross-validation\n",
    "\n",
    "transformed_train = selector.transform(train_set) # trainset after feature selection\n",
    "transformed_test = selector.transform(test_set)  # testset after feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.624407Z",
     "start_time": "2020-03-15T22:23:12.699Z"
    }
   },
   "outputs": [],
   "source": [
    "selector.best_sol # return the best solution feature index;\n",
    "selector.best_loss; # return the loss associated with the best solution; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.625810Z",
     "start_time": "2020-03-15T22:23:12.704Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from GA import Genetic_Algorithm # import the python script with GA.\n",
    "\n",
    "# classification example\n",
    "# use iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# random suffle the dataset\n",
    "# use the first 100 samples as train_set\n",
    "# the rest 5 samples as test_set\n",
    "idx = np.random.permutation(len(X))\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "train_set = X[0:100,:]\n",
    "test_set = X[100:,]\n",
    "train_y = y[0:100]\n",
    "test_y = y[100:,]\n",
    "\n",
    "# define a predictive model\n",
    "from sklearn.ensemble import ExtraTreesClassifier # we use extratree as predictive model\n",
    "\n",
    "# define the loss function in SA algorithm\n",
    "from sklearn.metrics import log_loss # for classification we use entropy\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=25)\n",
    "selector = Genetic_Algorithm(loss_func = log_loss, estimator = clf, \n",
    "                             n_gen = 5, n_pop = 10, predict_type = 'predict_proba')\n",
    "# parameters detail can be viewed from GA.py\n",
    "\n",
    "selector.fit(X_train = train_set, y_train = train_y, X_val = test_set, \n",
    "             y_val = test_y) # use pre-defined validation set\n",
    "\n",
    "transformed_train = selector.transform(train_set)  # trainset after feature selection\n",
    "transformed_test = selector.transform(test_set)  # testset after feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.627105Z",
     "start_time": "2020-03-15T22:23:12.709Z"
    }
   },
   "outputs": [],
   "source": [
    "selector.best_sol # return the best solution feature index;\n",
    "selector.best_loss; # return the loss associated with the best solution; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedded Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-25T21:07:27.865808Z",
     "start_time": "2020-01-25T21:07:27.860826Z"
    }
   },
   "source": [
    "The selection process of Filter Methods is independent of the ML models, so Filter Methods might select features that are less important in the ML models and might lead to poor model performance.\n",
    "\n",
    "Wrapper Methods utilize the predefined ML models to select the best features. But since they need to train models many times on a large number of possible subsets, they require long processing time although they usually lead to better performance than the Filter Methods.\n",
    "\n",
    "Embedded Methods embeds the feature selection process inside the ML models. They learn the best feature subset while the model is being created. So compared with the Filter Methods, they tend to have better performance. Compared with the Wrapper Methods, they save large processing time and computing power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparision between three approaches**.  \n",
    "   \n",
    "|Aspects | Filter Methods | Wrapper Methods\t| Embedded Methods\n",
    "|--|--|--|--|\n",
    "|Reliance on Model| No | Yes | Yes |\n",
    "|Requirement on Cross Validation |\tNo | Yes | Maybe\n",
    "|Process Time |\tShort | Long | Medium\n",
    "|Restriction on the ML models|\tNo | No | Yes (linear models with L1 or L2 norm or tree-based models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./images/Embedded_Pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regulization Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many ML models introduce penalties (L1 norm or L2 norm) in their loss functions to prevent the overfitting problem. The L1 norm penalization in linear models (such as Linear SVC, Logistic Regression, Linear Regression) tends to shrink the feature coefficients of some features to zero therefore results in sparse solutions. The L2 norm penalization can also shrink the feature coefficients but less likely to shrink coefficients to zero.\n",
    "\n",
    "We can use sklearn SelectFromModel function to remove features that have low or zero feature coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Regression (Linear Regression with L1 Norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.628786Z",
     "start_time": "2020-03-15T22:23:12.717Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso # we can also use Ridge regression with L2 norm \n",
    "\n",
    "# regression example\n",
    "# load the dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "dataset = fetch_california_housing()\n",
    "X, y = dataset.data, dataset.target # use california_housing dataset as example\n",
    "\n",
    "# use the first 15000 samples as train_set\n",
    "# the rest samples as test_set\n",
    "train_set = X[0:15000,:]\n",
    "test_set = X[15000:,]\n",
    "train_y = y[0:15000]\n",
    "\n",
    "clf = Lasso(normalize=True, alpha = 0.001)  # we need to normalize the data first or \n",
    "# the coefficient is meanless and uncomparable\n",
    "# if alpha is set too high, then the L1 norm will shrink every coefficients to 0\n",
    "# the bigger alpha, the stronger the penalty\n",
    "\n",
    "clf.fit(train_set, train_y)\n",
    "clf.coef_ # should select the 1st, 2nd, 7th features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.630833Z",
     "start_time": "2020-03-15T22:23:12.722Z"
    }
   },
   "outputs": [],
   "source": [
    "selector = SelectFromModel(clf, prefit=True, threshold=1e-5)\n",
    "# we can also set the max_features parameters to select the top features\n",
    "# threshold is set to 1e-5, so the features with absolut coefficients below 1e-5 \n",
    "# will be removed\n",
    "\n",
    "transformed_train = selector.transform(train_set)\n",
    "transformed_test = selector.transform(test_set)\n",
    "assert np.array_equal(transformed_train, train_set[:,[0,1,6]]) \n",
    "# select the 1st, 2nd, 7th features\n",
    "assert np.array_equal(transformed_test, test_set[:,[0,1,6]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression (with L1 Norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.632360Z",
     "start_time": "2020-03-15T22:23:12.732Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris # use iris dataset as example\n",
    "\n",
    "# use iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# random suffle the dataset\n",
    "# use the first 100 samples as train_set\n",
    "# the rest 5 samples as test_set\n",
    "np.random.seed(1234)\n",
    "idx = np.random.permutation(len(X))\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "# use the first 100 samples as train_set\n",
    "# the rest 5 samples as test_set\n",
    "train_set = X[0:100,:]\n",
    "test_set = X[100:,]\n",
    "train_y = y[0:100]\n",
    "\n",
    "# we need to standardize the data first or the coefficient is useless and uncomparable\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "model = StandardScaler()\n",
    "model.fit(train_set) \n",
    "standardized_train = model.transform(train_set)\n",
    "standardized_test = model.transform(test_set)\n",
    "\n",
    "clf = LogisticRegression(penalty='l1', C = 0.7, random_state=1234) \n",
    "# can set the penalty to 'l2'\n",
    "# if C is set too low, then the L1 norm will shrink every coefficients to 0\n",
    "# the bigger C, the weaker the penalty\n",
    "\n",
    "clf.fit(standardized_train, train_y)\n",
    "clf.coef_ # should select the 2nd, 3rd and 4th features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.634142Z",
     "start_time": "2020-03-15T22:23:12.737Z"
    }
   },
   "outputs": [],
   "source": [
    "selector = SelectFromModel(clf, prefit=True, threshold=1e-5)\n",
    "# we can also set the max_features parameters to select the top features\n",
    "# threshold is set to 1e-5, so the features with absolut coefficients below 1e-5 \n",
    "# will be removed\n",
    "\n",
    "transformed_train = selector.transform(train_set)\n",
    "transformed_test = selector.transform(test_set)\n",
    "assert np.array_equal(transformed_train, train_set[:,[1,2,3]]) \n",
    "# select the 2nd, 3rd features\n",
    "assert np.array_equal(transformed_test, test_set[:,[1,2,3]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVR/ LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.636118Z",
     "start_time": "2020-03-15T22:23:12.742Z"
    }
   },
   "outputs": [],
   "source": [
    "# we can use LinearSVC for classification\n",
    "# Or LinearSVR for regression\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "# regression example\n",
    "# load the dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "dataset = fetch_california_housing()\n",
    "X, y = dataset.data, dataset.target # use california_housing dataset as example\n",
    "\n",
    "# use the first 15000 samples as train_set\n",
    "# the rest samples as test_set\n",
    "train_set = X[0:15000,:]\n",
    "test_set = X[15000:,]\n",
    "train_y = y[0:15000]\n",
    "\n",
    "# we need to standardize the data first or the coefficient is useless and uncomparable\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "model = StandardScaler()\n",
    "model.fit(train_set) \n",
    "standardized_train = model.transform(train_set)\n",
    "standardized_test = model.transform(test_set)\n",
    "\n",
    "clf = LinearSVR(C = 0.0001, random_state = 123) \n",
    "# the bigger C, the weaker the penalty\n",
    "\n",
    "clf.fit(standardized_train, train_y)\n",
    "clf.coef_ # only remove the 6th feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.637340Z",
     "start_time": "2020-03-15T22:23:12.746Z"
    }
   },
   "outputs": [],
   "source": [
    "selector = SelectFromModel(clf, prefit=True, threshold=1e-2)\n",
    "# we can also set the max_features parameters to select the top features\n",
    "# threshold is set to 1e-2, so the features with absolut coefficients below 1e-2\n",
    "# will be removed\n",
    "\n",
    "transformed_train = selector.transform(train_set)\n",
    "transformed_test = selector.transform(test_set)\n",
    "assert np.array_equal(transformed_train, train_set[:,[0,1,2,3,4,6,7]]) \n",
    "# only remove the 6th feature\n",
    "assert np.array_equal(transformed_test, test_set[:,[0,1,2,3,4,6,7]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many powerful tree-based ML models such as random forest, AdaBoost, Xgboost, etc. You can check out more introduction about these tree-based ML models in a series of blogs written by my friend and me [here](https://github.com/YC-Coder-Chen/Tree-Math). Â \n",
    "Â  \n",
    "These non-parametric models record how each feature is used to reduce the loss by splitting the tree nodes and can report the feature importances of each feature based on the above records. The feature importances can be used to discard irrelevant features.Â "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.638499Z",
     "start_time": "2020-03-15T22:23:12.752Z"
    }
   },
   "outputs": [],
   "source": [
    "# we use the random forest regressor model as the example\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# load the dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "dataset = fetch_california_housing()\n",
    "X, y = dataset.data, dataset.target # use california_housing dataset as example\n",
    "\n",
    "# use the first 15000 samples as train_set\n",
    "# the rest samples as test_set\n",
    "train_set = X[0:15000,:]\n",
    "test_set = X[15000:,]\n",
    "train_y = y[0:15000]\n",
    "\n",
    "# we don't need to normaliz the dataset in tree-based feature selection\n",
    "\n",
    "clf = RandomForestRegressor(n_estimators = 50, random_state = 123)\n",
    "\n",
    "clf.fit(train_set, train_y)\n",
    "clf.feature_importances_ # should only remove the 4th features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.639852Z",
     "start_time": "2020-03-15T22:23:12.758Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the variable importance\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='seagreen', align='center')\n",
    "plt.yticks(range(len(indices)),np.array(dataset.feature_names)[indices])\n",
    "plt.xlabel('Relative Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:39.641491Z",
     "start_time": "2020-03-15T22:23:12.766Z"
    }
   },
   "outputs": [],
   "source": [
    "selector = SelectFromModel(clf, prefit=True, threshold='median')\n",
    "# we can also set the max_features parameters to select the top features\n",
    "# threshold is set to 'median', which is the median of the variable importances\n",
    "# which is around 0.0763\n",
    "\n",
    "transformed_train = selector.transform(train_set)\n",
    "transformed_test = selector.transform(test_set)\n",
    "assert np.array_equal(transformed_train, train_set[:,[0,5,6,7]]) \n",
    "# select the 1st, 6th, 7th and 8th features\n",
    "assert np.array_equal(transformed_test, test_set[:,[0,5,6,7]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "690.469px",
    "left": "20.7188px",
    "top": "136.836px",
    "width": "312.969px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
