{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T05:01:05.876506Z",
     "start_time": "2020-05-27T05:00:38.841773Z"
    }
   },
   "outputs": [],
   "source": [
    "# package imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 相关全局设置\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 只读取部分数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('big_data.csv', skiprows = lambda x: x > 0 and np.random.rand() > 0.01)\n",
    "\n",
    "print('it has been reduced 100 times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 减少不必要的数据内存\n",
    "读取数据后使用一次，在数据进入模型训练前，再使用一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T02:59:14.334115Z",
     "start_time": "2020-04-16T02:59:14.291406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 1 µs, total: 8 µs\n",
      "Wall time: 28.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#这个前面用\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem/(1024*1024)))\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem/(1024*1024)))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "#这个只能最后一步用，因为有些数据处理不接受category\n",
    "def reduce_mem_usage1(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem/(1024*1024)))\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem/(1024*1024)))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 类别特征分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看类别个数及其分布情况\n",
    "类别倾斜非常严重的可以删掉，比如：seller、offerType\n",
    "\n",
    "有异常值的要处理，比如：notRepairedDamage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'name'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "708       282\n",
       "387       282\n",
       "55        280\n",
       "1541      263\n",
       "203       233\n",
       "         ... \n",
       "5074        1\n",
       "7123        1\n",
       "11221       1\n",
       "13270       1\n",
       "174485      1\n",
       "Name: name, Length: 99662, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'model'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0      11762\n",
       "19.0      9573\n",
       "4.0       8445\n",
       "1.0       6038\n",
       "29.0      5186\n",
       "         ...  \n",
       "245.0        2\n",
       "209.0        2\n",
       "240.0        2\n",
       "242.0        2\n",
       "247.0        1\n",
       "Name: model, Length: 248, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'brand'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0     31480\n",
       "4     16737\n",
       "14    16089\n",
       "10    14249\n",
       "1     13794\n",
       "6     10217\n",
       "9      7306\n",
       "5      4665\n",
       "13     3817\n",
       "11     2945\n",
       "3      2461\n",
       "7      2361\n",
       "16     2223\n",
       "8      2077\n",
       "25     2064\n",
       "27     2053\n",
       "21     1547\n",
       "15     1458\n",
       "19     1388\n",
       "20     1236\n",
       "12     1109\n",
       "22     1085\n",
       "26      966\n",
       "30      940\n",
       "17      913\n",
       "24      772\n",
       "28      649\n",
       "32      592\n",
       "29      406\n",
       "37      333\n",
       "2       321\n",
       "31      318\n",
       "18      316\n",
       "36      228\n",
       "34      227\n",
       "33      218\n",
       "23      186\n",
       "35      180\n",
       "38       65\n",
       "39        9\n",
       "Name: brand, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'bodyType'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0    41420\n",
       "1.0    35272\n",
       "2.0    30324\n",
       "3.0    13491\n",
       "4.0     9609\n",
       "5.0     7607\n",
       "6.0     6482\n",
       "7.0     1289\n",
       "Name: bodyType, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'fuelType'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0    91656\n",
       "1.0    46991\n",
       "2.0     2212\n",
       "3.0      262\n",
       "4.0      118\n",
       "5.0       45\n",
       "6.0       36\n",
       "Name: fuelType, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'gearbox'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0    111623\n",
       "1.0     32396\n",
       "Name: gearbox, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'notRepairedDamage'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0    111361\n",
       "-       24324\n",
       "1.0     14315\n",
       "Name: notRepairedDamage, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'regionCode'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "419     369\n",
       "764     258\n",
       "125     137\n",
       "176     136\n",
       "462     134\n",
       "       ... \n",
       "6414      1\n",
       "7063      1\n",
       "4239      1\n",
       "5931      1\n",
       "7267      1\n",
       "Name: regionCode, Length: 7905, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'seller'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0    149999\n",
       "1         1\n",
       "Name: seller, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'offerType'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0    150000\n",
       "Name: offerType, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in cate_cols:\n",
    "    col\n",
    "    data_train[col].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据清洗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[feature].fillna('unknown', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 异常值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里我包装了一个异常值处理的代码，可以随便调用。\n",
    "def outliers_proc(data, col_name, scale=3):\n",
    "    \"\"\"\n",
    "    用于清洗异常值，默认用 box_plot（scale=3）进行清洗\n",
    "    :param data: 接收 pandas 数据格式\n",
    "    :param col_name: pandas 列名\n",
    "    :param scale: 尺度\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    def box_plot_outliers(data_ser, box_scale):\n",
    "        \"\"\"\n",
    "        利用箱线图去除异常值\n",
    "        :param data_ser: 接收 pandas.Series 数据格式\n",
    "        :param box_scale: 箱线图尺度，\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        iqr = box_scale * (data_ser.quantile(0.75) - data_ser.quantile(0.25))\n",
    "        val_low = data_ser.quantile(0.25) - iqr\n",
    "        val_up = data_ser.quantile(0.75) + iqr\n",
    "        rule_low = (data_ser < val_low)\n",
    "        rule_up = (data_ser > val_up)\n",
    "        return (rule_low, rule_up), (val_low, val_up)\n",
    "\n",
    "    data_n = data.copy()\n",
    "    data_series = data_n[col_name]\n",
    "    rule, value = box_plot_outliers(data_series, box_scale=scale)\n",
    "    index = np.arange(data_series.shape[0])[rule[0] | rule[1]]\n",
    "    print(\"Delete number is: {}\".format(len(index)))\n",
    "    data_n = data_n.drop(index)\n",
    "    data_n.reset_index(drop=True, inplace=True)\n",
    "    print(\"Now column number is: {}\".format(data_n.shape[0]))\n",
    "    index_low = np.arange(data_series.shape[0])[rule[0]]\n",
    "    outliers = data_series.iloc[index_low]\n",
    "    print(\"Description of data less than the lower bound is:\")\n",
    "    print(pd.Series(outliers).describe())\n",
    "    index_up = np.arange(data_series.shape[0])[rule[1]]\n",
    "    outliers = data_series.iloc[index_up]\n",
    "    print(\"Description of data larger than the upper bound is:\")\n",
    "    print(pd.Series(outliers).describe())\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 7))\n",
    "    sns.boxplot(y=data[col_name], data=data, palette=\"Set1\", ax=ax[0])\n",
    "    sns.boxplot(y=data_n[col_name], data=data_n, palette=\"Set1\", ax=ax[1])\n",
    "    return data_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T03:01:20.566318Z",
     "start_time": "2020-04-16T03:01:20.533649Z"
    }
   },
   "outputs": [],
   "source": [
    "# 以groupBy构造统计量\n",
    "def groupby_feature(col, target):\n",
    "    Train_gb = data_train.groupby(col)\n",
    "    all_info = {}\n",
    "    for kind, kind_data in Train_gb:\n",
    "        info = {}\n",
    "        kind_data = kind_data[kind_data[target] > 0]\n",
    "#         info[col+'_amount'] = len(kind_data)\n",
    "        info[col + '_' + target +'_max'] = kind_data[target].max()\n",
    "        info[col + '_' + target +'_median'] = kind_data[target].median()\n",
    "        info[col + '_' + target +'_min'] = kind_data[target].min()\n",
    "        info[col + '_' + target +'_sum'] = kind_data[target].sum()\n",
    "        info[col + '_' + target +'_std'] = kind_data[target].std()\n",
    "        info[col + '_' + target +'_average'] = round(kind_data[target].sum() / (len(kind_data) + 1), 2)\n",
    "        all_info[kind] = info\n",
    "    brand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={'index': col})\n",
    "    return data.merge(brand_fe, how='left', on=col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM\n",
    "\n",
    "1. https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
    "2. https://mathpretty.com/10649.html\n",
    "3. https://blog.csdn.net/u012735708/article/details/83749703"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from lightgbm import plot_importance\n",
    "\n",
    "\n",
    "lgb_train_data = lgb.Dataset(x_train, y_train, silent=True)\n",
    "\n",
    "params = { \n",
    "    #一般固定的参数\n",
    "    'objective': 'regression_l1', #=mae\n",
    "    'metric': 'mae',\n",
    "#     'categorical_feature': cate_feature,\n",
    "    'bagging_fraction': 0.8, # 数据采样\n",
    "    'feature_fraction': 0.8, # 特征采样\n",
    "\n",
    "    'verbosity': 1, #< 0: Fatal, = 0: Error (Warning), = 1: Info, > 1: Debug\n",
    "    'nthread' : 8,\n",
    "    \n",
    "    #调参一般改动的参数\n",
    "    'lambda_l1': 0,\n",
    "    'lambda_l2': 0,\n",
    "    'min_gain_to_split': 0.0,\n",
    "    \n",
    "    'learning_rate': 0.05, \n",
    "    'num_iterations' : 5000,\n",
    "    \n",
    "    'max_depth': 10,\n",
    "    'num_leaves': 100, #不要超过2^max_depth\n",
    "    \n",
    "    'max_bin': 255,\n",
    "    'seed': 2020,  #保证每次训练随机部分相同\n",
    "    'verbose': -1, #只打印错误信息\n",
    "    \n",
    "    }\n",
    "\n",
    "# 当train里面的参数与params里面的参数重复时，以params的为准\n",
    "# 若传入的数据是Dataframe，则会自动识别cat数据。或者自定义cat数据，然后指定为categorical_feature\n",
    "model = lgb.train(params, data = lgb_train_data, valid = [lgb_train_data,lgb_val_data], categorical_feature=cate_feature , verbose_eval=500, early_stopping_rounds = 50)\n",
    "\n",
    "# cv\n",
    "cv_results = lgb.cv(params, lgb_data_train, shuffle=True, early_stopping_rounds=50, verbose_eval=100, show_stdv=True)\n",
    "\n",
    "# 特征权重分析\n",
    "lgb.plot_importance(model)\n",
    "\n",
    "#预测\n",
    "y_pred = gbm.predict(x_test, num_iteration=model.best_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自定义函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40-50次训练\n",
    "# 自定义目标函数\n",
    "def loglikelihood(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    preds = 1. / (1. + np.exp(-preds))\n",
    "    grad = preds - labels\n",
    "    hess = preds * (1. - preds)\n",
    "    return grad, hess\n",
    "# 自定义评估函数\n",
    "def binary_error(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    return 'error', np.mean(labels != (preds > 0.5)), False\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "               lgb_train,\n",
    "                valid_sets=lgb_eval,\n",
    "               fobj=loglikelihood, # 自定义目标函数\n",
    "               feval=binary_error, # 自定义评估函数\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T02:56:30.013351Z",
     "start_time": "2020-04-16T02:56:29.990097Z"
    }
   },
   "source": [
    "### 学习曲线learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, score=None, n_jobs=1, \n",
    "                        train_sizes=np.linspace(0.1, 1.0, 5), verbose=0, plot=True):\n",
    "    \"\"\"\n",
    "    画出data在某模型上的learning curve.\n",
    "    参数解释\n",
    "    ----------\n",
    "    estimator : 你用的分类器。\n",
    "    title : 表格的标题。\n",
    "    X : 输入的feature，numpy类型\n",
    "    y : 输入的target vector\n",
    "    ylim : tuple格式的(ymin, ymax), 设定图像中纵坐标的最低点和最高点\n",
    "    cv : 做cross-validation的时候，数据分成的份数，其中一份作为cv集，其余n-1份作为training(默认为5份)\n",
    "    n_jobs : 并行的的任务数(默认1)\n",
    "    \"\"\"\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, scoring=score, n_jobs=n_jobs, train_sizes=train_sizes, verbose=verbose)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.title(title)\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.xlabel('trian number')\n",
    "        plt.ylabel('score')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(True)\n",
    "    \n",
    "        plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, \n",
    "                         alpha=0.1, color=\"b\")\n",
    "        plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, \n",
    "                         alpha=0.1, color=\"r\")\n",
    "        plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\", label='train score')\n",
    "        plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\", label='test score')\n",
    "    \n",
    "        plt.legend(loc='best')\n",
    "        \n",
    "        plt.draw()\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "    \n",
    "    midpoint = ((train_scores_mean[-1] + train_scores_std[-1]) + (test_scores_mean[-1] - test_scores_std[-1])) / 2\n",
    "    diff = (train_scores_mean[-1] + train_scores_std[-1]) - (test_scores_mean[-1] - test_scores_std[-1])\n",
    "    return midpoint, diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "              'max_depth': [15, 20, 25, 30, 35],\n",
    "              'learning_rate': [0.01, 0.02, 0.05, 0.1, 0.15],\n",
    "              'feature_fraction': [0.6, 0.7, 0.8, 0.9, 0.95],\n",
    "              'bagging_fraction': [0.6, 0.7, 0.8, 0.9, 0.95],\n",
    "              'bagging_freq': [2, 4, 5, 6, 8],\n",
    "              'lambda_l1': [0, 0.1, 0.4, 0.5, 0.6],\n",
    "              'lambda_l2': [0, 10, 15, 35, 40],\n",
    "              'cat_smooth': [1, 10, 15, 20, 35]\n",
    "}\n",
    "gbm = lgb.LGBMClassifier(boosting_type='gbdt',\n",
    "                         objective = 'binary',\n",
    "                         metric = 'auc',\n",
    "                         verbose = 0,\n",
    "                         learning_rate = 0.01,\n",
    "                         num_leaves = 35,\n",
    "                         feature_fraction=0.8,\n",
    "                         bagging_fraction= 0.9,\n",
    "                         bagging_freq= 8,\n",
    "                         lambda_l1= 0.6,\n",
    "                         lambda_l2= 0)\n",
    "# 有了gridsearch我们便不需要fit函数\n",
    "gsearch = GridSearchCV(gbm, param_grid=parameters, scoring='accuracy', cv=3)\n",
    "gsearch.fit(train_x, train_y)\n",
    "\n",
    "print(\"Best score: %0.3f\" % gsearch.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = gsearch.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 贝叶斯调参\n",
    "\n",
    "下面以lgb为例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def lgb_cv(feature_fraction,bagging_fraction,min_data_in_leaf,max_depth,min_split_gain,num_leaves,lambda_l1,lambda_l2,num_iterations=1000):\n",
    "        params = {'objective': 'regression_l1','num_iterations': num_iterations, 'early_stopping_round':50}\n",
    "        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "        params[\"min_data_in_leaf\"] = int(round(min_data_in_leaf))\n",
    "        params['max_depth'] = int(round(max_depth))\n",
    "        params['min_split_gain'] = min_split_gain      \n",
    "        params[\"num_leaves\"] = int(round(num_leaves))\n",
    "        params['lambda_l1'] = max(lambda_l1, 0)\n",
    "        params['lambda_l2'] = max(lambda_l2, 0)\n",
    "        \n",
    "        cv_result = lgb.cv(params, lgb_data_train, nfold=5, seed=2, stratified=False)\n",
    "        return -(min(cv_result['l1-mean']))\n",
    "\n",
    "lgb_opt = BayesianOptimization(\n",
    "        lgb_cv,\n",
    "        {'feature_fraction': (0.5, 1),\n",
    "        'bagging_fraction': (0.5, 1),\n",
    "        'min_data_in_leaf': (1,100),\n",
    "        'max_depth': (6, 15),\n",
    "         'min_split_gain': (0, 5),\n",
    "         'num_leaves': (50, 128),\n",
    "         'lambda_l1': (0, 100),\n",
    "         'lambda_l2': (0, 100)}\n",
    "    )        \n",
    "\n",
    "lgb_opt.maximize(init_points=21,n_iter=90) #init_points表示初始点，n_iter代表迭代次数（即采样数）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型融合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "models=[\n",
    "    ('lgb', model = LGBMRegressor(num_leaves=100,\n",
    "                          max_depth=10,\n",
    "                          learning_rate=0.05,\n",
    "                          n_estimators=5000,\n",
    "                          bagging_fraction=0.8,\n",
    "                          feature_fraction=0.8,\n",
    "                          random_state=15,\n",
    "                          metric='mae',\n",
    "                          )),\n",
    "    ('log_clf', LogisticRegression()),\n",
    "    ('svm_clf', SVC(probability=True)),\n",
    "    ('rf_clf', RandomForestClassifier()),\n",
    "    ('gbdt_clf', GradientBoostingClassifier())\n",
    "]\n",
    "\n",
    "\n",
    "model = StackingClassifier(estimators=models, final_estimator=LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提交结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['SaleID'] = data_testA['SaleID']\n",
    "sub['price'] = result\n",
    "sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# magic command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T02:59:55.129051Z",
     "start_time": "2020-04-16T02:59:55.121007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "112px",
    "width": "333px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
