{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T02:32:54.315731Z",
     "start_time": "2020-07-23T02:32:50.105496Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import datetime \n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision as tv\n",
    "import torchkeras\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建\n",
    "\n",
    "张量的数据类型和numpy.array基本一一对应，但是不支持str类型。\n",
    "\n",
    "包括:\n",
    "\n",
    "torch.float64(torch.double),\n",
    "\n",
    "torch.float32(torch.float),\n",
    "\n",
    "torch.float16,\n",
    "\n",
    "torch.int64(torch.long),\n",
    "\n",
    "torch.int32(torch.int),\n",
    "\n",
    "torch.int16,\n",
    "\n",
    "torch.int8,\n",
    "\n",
    "torch.uint8,\n",
    "\n",
    "torch.bool\n",
    "\n",
    "一般神经网络建模使用的都是torch.float32类型。\n",
    "\n",
    "\n",
    "首先，我们需要明确一下，torch.Tensor()是python类，更明确地说，是默认张量类型torch.FloatTensor()的别名，torch.Tensor([1,2])会调用Tensor类的构造函数init，生成单精度浮点类型的张量。而torch.tensor()是函数。我们通常使用torch.tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T08:54:01.124045Z",
     "start_time": "2020-07-06T08:54:01.105958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 10, 13],\n",
       "        [ 2,  3,  9],\n",
       "        [14,  2,  4],\n",
       "        [ 4, 14, 12],\n",
       "        [14,  3, 14]], dtype=torch.int8)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Construct a tensor directly from data:\n",
    "x = torch.tensor([5.5, 3])\n",
    "\n",
    "#Construct a 5x3 matrix, uninitialized:\n",
    "x = torch.empty(5, 3)\n",
    "\n",
    "#Construct a randomly initialized matrix:\n",
    "x = torch.rand(5, 3)\n",
    "\n",
    "#Construct a matrix filled zeros and of dtype long\n",
    "#pytorch dtype:https://pytorch.org/docs/stable/tensor_attributes.html\n",
    "x = torch.randint(15, (5, 3),dtype=torch.int8)\n",
    "x\n",
    "#create a tensor based on an existing tensor.\n",
    "t = torch.ones_like(x, dtype=torch.double)      # new_* methods take in sizes\n",
    "t\n",
    "\n",
    "\n",
    "#基于GPU版本\n",
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T04:33:59.402942Z",
     "start_time": "2020-07-08T04:33:59.397855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 2.5244e-29, 2.5457e-19],\n",
       "        [2.5250e-29, 8.4078e-45, 0.0000e+00]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = torch.Tensor(2,3)\n",
    "aa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T08:54:01.582988Z",
     "start_time": "2020-07-06T08:54:01.574526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 13],\n",
       "        [ 3,  9],\n",
       "        [ 2,  4],\n",
       "        [14, 12],\n",
       "        [ 3, 14]], dtype=torch.int8)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#与numpy索引一样\n",
    "x[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T08:54:01.908799Z",
     "start_time": "2020-07-06T08:54:01.892512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.8584, 10.6993, 13.2126],\n",
       "        [ 2.9645,  3.2152,  9.1376],\n",
       "        [14.2607,  2.5452,  4.3598],\n",
       "        [ 4.3055, 14.4265, 12.1328],\n",
       "        [14.6075,  3.3441, 14.1507]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[10.8584, 10.6993, 13.2126],\n",
       "        [ 2.9645,  3.2152,  9.1376],\n",
       "        [14.2607,  2.5452,  4.3598],\n",
       "        [ 4.3055, 14.4265, 12.1328],\n",
       "        [14.6075,  3.3441, 14.1507]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[10.8584, 10.6993, 13.2126],\n",
       "        [ 2.9645,  3.2152,  9.1376],\n",
       "        [14.2607,  2.5452,  4.3598],\n",
       "        [ 4.3055, 14.4265, 12.1328],\n",
       "        [14.6075,  3.3441, 14.1507]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[10, 10, 13],\n",
       "        [ 2,  3,  9],\n",
       "        [14,  2,  4],\n",
       "        [ 4, 14, 12],\n",
       "        [14,  3, 14]], dtype=torch.int8)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5,3)\n",
    "x+y\n",
    "torch.add(x,y)\n",
    "#Any operation that mutates a tensor in-place is post-fixed with an _. For example: x.copy_(y), x.t_(), will change x.\n",
    "y.add_(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor<->numpy\n",
    "The Torch Tensor and NumPy array will share their underlying memory locations (if the Torch Tensor is on CPU), and changing one will change the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T08:54:06.406749Z",
     "start_time": "2020-07-06T08:54:06.383513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2.], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a\n",
    "b = a.numpy()\n",
    "b\n",
    "c = torch.from_numpy(b)\n",
    "c\n",
    "a.add_(1)\n",
    "a\n",
    "b\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他常用函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 尺寸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T12:42:18.710937Z",
     "start_time": "2020-07-20T12:42:18.653041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "torch.Size([12])\n",
      "torch.Size([12])\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "torch.Size([3, 4])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "False\n",
      "tensor([[ 0,  6,  1,  7],\n",
      "        [ 2,  8,  3,  9],\n",
      "        [ 4, 10,  5, 11]])\n"
     ]
    }
   ],
   "source": [
    "#查看尺寸\n",
    "vector = torch.arange(0,12)\n",
    "print(vector)\n",
    "print(vector.shape)\n",
    "print(vector.size())\n",
    "#使用view改变尺寸\n",
    "matrix34 = vector.view(3,4)\n",
    "print(matrix34)\n",
    "print(matrix34.shape)\n",
    "\n",
    "# 有些操作会让张量存储结构扭曲，直接使用view会失败，可以用reshape方法\n",
    "\n",
    "matrix26 = torch.arange(0,12).view(2,6)\n",
    "print(matrix26)\n",
    "print(matrix26.shape)\n",
    "\n",
    "# 转置操作让张量存储结构扭曲\n",
    "matrix62 = matrix26.t()\n",
    "print(matrix62.is_contiguous())\n",
    "\n",
    "\n",
    "# 直接使用view方法会失败，可以使用reshape方法\n",
    "#matrix34 = matrix62.view(3,4) #error!\n",
    "matrix34 = matrix62.reshape(3,4) #等价于matrix34 = matrix62.contiguous().view(3,4)\n",
    "print(matrix34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 元素个数，reshape的需要考量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T12:42:20.245846Z",
     "start_time": "2020-07-20T12:42:20.237079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix26.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T12:52:56.139334Z",
     "start_time": "2020-07-20T12:52:56.130895Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3]\n",
    "], dtype=torch.float32)\n",
    "t = t.reshape(2,2,3)\n",
    "t.reshape([1,12]).squeeze().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "\n",
    "一个Tensor中通常会记录如下图中所示的属性：\n",
    "\n",
    "- data: 即存储的数据信息\n",
    "- requires_grad: 设置为True则表示该Tensor需要求导\n",
    "- grad: 该Tensor的梯度值，每次在计算backward时都需要将前一时刻的梯度归零，否则梯度值会一直累加，这个会在后面讲到。\n",
    "- grad_fn: 叶子节点通常为None，只有结果节点的grad_fn才有效，用于指示梯度函数是哪种类型。例如上面示例代码中的y.grad_fn=<PowBackward0 at 0x213550af048>, z.grad_fn=<AddBackward0 at 0x2135df11be0>\n",
    "- is_leaf: 用来指示该Tensor是否是叶子节点。\n",
    "\n",
    "在反向传播过程中，只有 is_leaf=True 的叶子节点，也就是需要求导的张量的导数结果才会被最后保留下来。\n",
    "\n",
    "那么什么是叶子节点张量呢？叶子节点张量需要满足两个条件。\n",
    "\n",
    "1，叶子节点张量是由用户直接创建的张量，而非由某个Function通过计算得到的张量。\n",
    "\n",
    "2，叶子节点张量的 requires_grad属性必须为True.\n",
    "\n",
    "Pytorch设计这样的规则主要是为了节约内存或者显存空间，因为几乎所有的时候，用户只会关心他自己直接创建的张量的梯度。\n",
    "\n",
    "所有依赖于叶子节点张量的张量, 其requires_grad 属性必定是True的，但其梯度值只在计算过程中被用到，不会最终存储到grad属性中。\n",
    "\n",
    "如果需要保留中间计算结果的梯度到grad属性中，可以使用 retain_grad方法。 如果仅仅是为了调试代码查看梯度值，可以利用register_hook打印日志。\n",
    "\n",
    "reference：\n",
    "1. https://zhuanlan.zhihu.com/p/83172023\n",
    "2. https://github.com/lyhue1991/eat_pytorch_in_20_days/blob/master/2-3,%E5%8A%A8%E6%80%81%E8%AE%A1%E7%AE%97%E5%9B%BE.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scalar gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T10:07:13.470288Z",
     "start_time": "2020-07-06T10:07:13.462740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<PowBackward0 at 0x7f94311f3910>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#我们自己定义的变量，我们称之为叶子张量(leaf Tensor)，而基于叶子节点得到的中间或最终变量则可称之为结果张量\n",
    "#这里X是leaf Tensor、其他的都是结果张量\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = x**2\n",
    "# z = x + x\n",
    "\n",
    "y.backward()\n",
    "x.grad\n",
    "#只能对leaf Tensor求梯度，下面代码会报错\n",
    "#y.grad\n",
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matrix gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T09:53:54.688821Z",
     "start_time": "2020-07-06T09:53:54.679330Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x7f94311df290>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2,requires_grad=True)\n",
    "z = x + 2\n",
    "z.backward(torch.ones_like(z))\n",
    "\n",
    "x.grad\n",
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度是累加的\n",
    "\n",
    "正因为求导链式法则衍生的梯度累加规则，张量的grad梯度不会自动清零，在需要的时候需要手动置零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#梯度是累加的。可以发现我们的w初始为-0.8820,计算出来的导数为-47.0557 第2行是backward计算的，第3行是我们自己计算的。但是发现下面的第4、5行和想象的不一样，手动计算的好像没问题还是-47.0557，但为什么backward计算的变成了-94.1115,似乎是-47.0557*2,其实是因为pytorch设计就是这样的，它的梯度默认会保存累加，所以这次的梯度是我们这次的加上上一次的。讲到这里，估计也就明白了为什么我们在进行梯度下降的时候需要用optimizer.zero_grad()了，就是要清空梯度。\n",
    "x = torch.tensor([2])\n",
    "y = torch.tensor([10])\n",
    "loss = torch.nn.MSELoss()\n",
    "w = torch.randn(1,requires_grad=True)\n",
    "for i in range(10):\n",
    "    print('w = ',w)\n",
    "    y_ = w*x\n",
    "    l = loss(y,y_)\n",
    "    print('loss = ', l)\n",
    "    l.backward()\n",
    "    print('w grad = ',w.grad)\n",
    "\n",
    "#上面的例子只是计算了梯度，并没有进行梯度更新。下面加入更新梯度。（因为这里我重新定义了w，相当于清空了梯度，所以w的梯度不会累加了）\n",
    "w = torch.randn(1,requires_grad=True)\n",
    "# print(w)\n",
    "for i in range(10):\n",
    "    print('w = ',w)\n",
    "    y_ = w*x\n",
    "    l = loss(y,y_)\n",
    "    print('loss = ', l)\n",
    "    l.backward()\n",
    "    print('w grad = ',w.grad)\n",
    "    w = torch.tensor(w - 0.1 * w.grad.data, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset、DataLoader\n",
    "https://github.com/lyhue1991/eat_pytorch_in_20_days/blob/master/5-1,Dataset%E5%92%8CDataLoader.md\n",
    "\n",
    "\n",
    "如何使用dataset：自定义class，继承dataset，实现len、getitem方法。在init方法里，定义自己额外的处理逻辑，比如transformer：图片翻转等\n",
    "\n",
    "如何使用dataloader：把dataset传进入，让Dataset变得可迭代。定义batchSize。比如训练数据有5条，batchSize为2，那么Dataloader就是[2,2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T05:03:55.370478Z",
     "start_time": "2020-07-09T05:03:55.349665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image name: 10comm-decarlo.jpg\n",
      "Landmarks shape: (68, 2)\n",
      "First 4 Landmarks: [[ 66. 114.]\n",
      " [ 65. 128.]\n",
      " [ 67. 142.]\n",
      " [ 68. 156.]]\n"
     ]
    }
   ],
   "source": [
    "from skimage import io, transform\n",
    "landmarks_frame = pd.read_csv('data/faces/face_landmarks.csv')\n",
    "\n",
    "n = 2\n",
    "img_name = landmarks_frame.iloc[n, 0]\n",
    "landmarks = landmarks_frame.iloc[n, 1:]\n",
    "\n",
    "landmarks = np.asarray(landmarks)\n",
    "landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "\n",
    "print('Image name: {}'.format(img_name))\n",
    "print('Landmarks shape: {}'.format(landmarks.shape))\n",
    "print('First 4 Landmarks: {}'.format(landmarks[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceLandmarksDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.landmarks_frame.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        landmarks = np.array([landmarks])\n",
    "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "def get_data(train_ds, bs):\n",
    "    return DataLoader(train_ds, batch_size=bs)\n",
    "\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl= get_data(train_ds, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现简单的NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:24:08.402168Z",
     "start_time": "2020-07-06T06:24:08.087720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30868888.97226157\n",
      "50 15115.76309242181\n",
      "100 668.9667192362771\n",
      "150 47.77611498273488\n",
      "200 4.117628487738955\n",
      "250 0.39061195227189\n",
      "300 0.03923053182559015\n",
      "350 0.004092673947545223\n",
      "400 0.00043904978012960187\n",
      "450 4.8150229161684405e-05\n"
     ]
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    #forward\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    #loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if(t%50 == 0):\n",
    "        print(t, loss)\n",
    "    \n",
    "    #backward\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    #update weight\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor NN（自动求导）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T10:22:25.980428Z",
     "start_time": "2020-07-06T10:22:25.619226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31319074.0\n",
      "50 11566.169921875\n",
      "100 335.9598693847656\n",
      "150 16.663705825805664\n",
      "200 1.0278314352035522\n",
      "250 0.0703810453414917\n",
      "300 0.005294266156852245\n",
      "350 0.0006232215091586113\n",
      "400 0.00014879746595397592\n",
      "450 5.6777495046844706e-05\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    #forward\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    #loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if(t%50 == 0):\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    #backward\n",
    "    loss.backward()\n",
    "    \n",
    "    #update weight\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn module（构建网络层，使用策略自动更新weight）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T11:26:29.072192Z",
     "start_time": "2020-07-06T11:26:28.475454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 643.749755859375\n",
      "50 205.24661254882812\n",
      "100 56.13291931152344\n",
      "150 10.085933685302734\n",
      "200 1.038871169090271\n",
      "250 0.07492955774068832\n",
      "300 0.004597858525812626\n",
      "350 0.0002783302334137261\n",
      "400 1.5107957551663276e-05\n",
      "450 6.134810632829613e-07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D_in, H), #nn.Linear根据我们给的网络维度，帮我们随机生成合理的w\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    #forward\n",
    "    y_pred=model(x)\n",
    "    \n",
    "    #loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if(t%50==0):\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    #backward    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #update weight\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T11:36:33.427372Z",
     "start_time": "2020-07-06T11:36:33.421735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Custom nn Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T03:22:50.444512Z",
     "start_time": "2020-07-08T03:22:50.004319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 687.6841430664062\n",
      "50 29.253353118896484\n",
      "100 1.5906261205673218\n",
      "150 0.16632196307182312\n",
      "200 0.023523369804024696\n",
      "250 0.003940855618566275\n",
      "300 0.0007560442318208516\n",
      "350 0.0001624409487703815\n",
      "400 3.8227102777455e-05\n",
      "450 9.624174708733335e-06\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super().__init__()\n",
    "        #定义模型的框架（有几层，每一层的结构等等）\n",
    "        self.linear1=nn.Linear(D_in, H)\n",
    "        self.linear2=nn.Linear(H, D_out)\n",
    "    \n",
    "    #定义forward过程\n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "    \n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    #forward\n",
    "    y_pred=model(x)#等价于调用model.__call__(x)，call函数里再调用的forward，同时会做一些额外操作\n",
    "    \n",
    "    #loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if(t%50==0):\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    #backward\n",
    "    loss.backward()\n",
    "    #update weight\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型的两种方法\n",
    "\n",
    "https://github.com/lyhue1991/eat_pytorch_in_20_days/blob/master/6-2,%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%843%E7%A7%8D%E6%96%B9%E6%B3%95.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T02:35:14.963673Z",
     "start_time": "2020-07-23T02:35:13.970110Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9bcb400290>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 下载数字图片\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "import gzip\n",
    "from matplotlib import pyplot\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "        \n",
    "pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T02:35:16.280123Z",
     "start_time": "2020-07-23T02:35:16.148076Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " torch.Size([50000, 784]),\n",
       " tensor(0),\n",
       " tensor(9))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
      "torch.Size([50000, 784])\n",
      "tensor(0) tensor(9)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "n, c = x_train.shape\n",
    "x_train, x_train.shape, y_train.min(), y_train.max()\n",
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原生态手写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T02:35:59.547832Z",
     "start_time": "2020-07-23T02:35:17.384217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "================================================================================2020-07-23 10:35:17\n",
      "0 (0.46063539385795593, 16) 1.1493806097030639\n",
      "1 (0.24581722915172577, 16) 0.5029971175432205\n",
      "2 (0.41273295879364014, 16) 0.5554273410558701\n",
      "3 (0.31130915880203247, 16) 0.37095076718330383\n",
      "4 (0.08998090773820877, 16) 0.23804985910952092\n",
      "5 (0.07402752339839935, 16) 0.23855782371163367\n",
      "6 (0.41513484716415405, 16) 0.19406586377620696\n",
      "7 (0.09942737966775894, 16) 0.15995553736537696\n",
      "8 (0.37536564469337463, 16) 0.42625115897655486\n",
      "9 (0.45665302872657776, 16) 0.38876693365573883\n"
     ]
    }
   ],
   "source": [
    "lr = 0.5  # learning rate\n",
    "epochs = 10  # how many epochs to train for\n",
    "bs = 64  # batch size\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "\n",
    "def accuracy(y_pred,y_true):\n",
    "    y_pred_cls = torch.argmax(nn.Softmax(dim=1)(y_pred),dim=1).data\n",
    "    return accuracy_score(y_true,y_pred_cls)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "metric_func = accuracy\n",
    "metric_name = \"accuracy\"\n",
    "\n",
    "dfhistory = pd.DataFrame(columns = [\"epoch\",\"loss\",metric_name,\"val_loss\",\"val_\"+metric_name]) \n",
    "print(\"Start Training...\")\n",
    "nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"==========\"*8 + \"%s\"%nowtime)\n",
    "\n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "class Mnist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(4,0)]\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x.view(-1, x.size(1))\n",
    "\n",
    "\n",
    "def loss_batch(model, loss_fn, xb, yb, opt=None):\n",
    "    loss = loss_fn(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def fit(epochs, model,loss_fn, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            opt.zero_grad()\n",
    "            loss_item = loss_batch(model, loss_fn, xb, yb, opt)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_fn, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "            \n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, loss_item, val_loss)\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model = Mnist()\n",
    "opt = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T02:36:28.304769Z",
     "start_time": "2020-07-23T02:36:28.297596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 14, 14]             160\n",
      "              ReLU-2           [-1, 16, 14, 14]               0\n",
      "            Conv2d-3             [-1, 16, 7, 7]           2,320\n",
      "              ReLU-4             [-1, 16, 7, 7]               0\n",
      "            Conv2d-5             [-1, 10, 4, 4]           1,450\n",
      "              ReLU-6             [-1, 10, 4, 4]               0\n",
      "         AvgPool2d-7             [-1, 10, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 3,930\n",
      "Trainable params: 3,930\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.002991\n",
      "Forward/backward pass size (MB): 0.062332\n",
      "Params size (MB): 0.014992\n",
      "Estimated Total Size (MB): 0.080315\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torchkeras.summary(model,input_shape=(1,1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T02:36:29.736500Z",
     "start_time": "2020-07-23T02:36:29.732613Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.weight \t\t torch.Size([16, 1, 3, 3])\n",
      "layers.0.bias \t\t torch.Size([16])\n",
      "layers.2.weight \t\t torch.Size([16, 16, 3, 3])\n",
      "layers.2.bias \t\t torch.Size([16])\n",
      "layers.4.weight \t\t torch.Size([10, 16, 3, 3])\n",
      "layers.4.bias \t\t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, '\\t\\t', param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用torchkeras（还有很多类型的三方库，做了一个更高的抽象）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T12:31:32.543166Z",
     "start_time": "2020-07-19T12:31:18.130271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training ...\n",
      "\n",
      "================================================================================2020-07-19 20:31:18\n",
      "{'step': 100, 'loss': 2.112, 'accuracy': 0.253}\n",
      "{'step': 200, 'loss': 1.945, 'accuracy': 0.322}\n",
      "{'step': 300, 'loss': 1.866, 'accuracy': 0.346}\n",
      "{'step': 400, 'loss': 1.81, 'accuracy': 0.361}\n",
      "{'step': 500, 'loss': 1.771, 'accuracy': 0.371}\n",
      "{'step': 600, 'loss': 1.74, 'accuracy': 0.378}\n",
      "{'step': 700, 'loss': 1.713, 'accuracy': 0.385}\n",
      "\n",
      " +-------+-------+----------+----------+--------------+\n",
      "| epoch |  loss | accuracy | val_loss | val_accuracy |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "|   1   | 1.695 |  0.389   |  1.508   |    0.436     |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "\n",
      "================================================================================2020-07-19 20:31:22\n",
      "{'step': 100, 'loss': 1.541, 'accuracy': 0.425}\n",
      "{'step': 200, 'loss': 1.529, 'accuracy': 0.429}\n",
      "{'step': 300, 'loss': 1.536, 'accuracy': 0.425}\n",
      "{'step': 400, 'loss': 1.534, 'accuracy': 0.426}\n",
      "{'step': 500, 'loss': 1.529, 'accuracy': 0.428}\n",
      "{'step': 600, 'loss': 1.527, 'accuracy': 0.428}\n",
      "{'step': 700, 'loss': 1.526, 'accuracy': 0.428}\n",
      "\n",
      " +-------+-------+----------+----------+--------------+\n",
      "| epoch |  loss | accuracy | val_loss | val_accuracy |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "|   2   | 1.522 |  0.429   |  1.471   |    0.445     |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "\n",
      "================================================================================2020-07-19 20:31:27\n",
      "{'step': 100, 'loss': 1.514, 'accuracy': 0.428}\n",
      "{'step': 200, 'loss': 1.507, 'accuracy': 0.429}\n",
      "{'step': 300, 'loss': 1.505, 'accuracy': 0.431}\n",
      "{'step': 400, 'loss': 1.503, 'accuracy': 0.433}\n",
      "{'step': 500, 'loss': 1.502, 'accuracy': 0.434}\n",
      "{'step': 600, 'loss': 1.497, 'accuracy': 0.436}\n",
      "{'step': 700, 'loss': 1.496, 'accuracy': 0.436}\n",
      "\n",
      " +-------+-------+----------+----------+--------------+\n",
      "| epoch |  loss | accuracy | val_loss | val_accuracy |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "|   3   | 1.494 |  0.436   |  1.456   |    0.444     |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "\n",
      "================================================================================2020-07-19 20:31:32\n",
      "Finished Training...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def accuracy(y_pred,y_true):\n",
    "    y_pred_cls = torch.argmax(nn.Softmax(dim=1)(y_pred),dim=1).data\n",
    "    return accuracy_score(y_true,y_pred_cls)\n",
    "\n",
    "model = torchkeras.Model(Mnist())\n",
    "model.compile(loss_func = nn.CrossEntropyLoss(),\n",
    "optimizer= torch.optim.Adam(model.parameters(),lr = 0.02),\n",
    "metrics_dict={\"accuracy\":accuracy})\n",
    "dfhistory = model.fit(3,dl_train = train_dl, dl_val=valid_dl, log_step_freq=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T12:31:40.718931Z",
     "start_time": "2020-07-19T12:31:40.701218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.695137</td>\n",
       "      <td>0.389186</td>\n",
       "      <td>1.508173</td>\n",
       "      <td>0.435621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.522427</td>\n",
       "      <td>0.429088</td>\n",
       "      <td>1.470825</td>\n",
       "      <td>0.444620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.494131</td>\n",
       "      <td>0.436041</td>\n",
       "      <td>1.456180</td>\n",
       "      <td>0.444422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  1.695137  0.389186  1.508173      0.435621\n",
       "1  1.522427  0.429088  1.470825      0.444620\n",
       "2  1.494131  0.436041  1.456180      0.444422"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfhistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/nn_tutorial.html#switch-to-cnn\n",
    "\n",
    "- torch.nn\n",
    "    - Module: creates a callable which behaves like a function, but can also contain state(such as neural net layer weights). It knows what Parameter (s) it contains and can zero all their gradients, loop through them for weight updates, etc.\n",
    "    - Parameter: a wrapper for a tensor that tells a Module that it has weights that need updating during backprop. Only tensors with the requires_grad attribute set are updated\n",
    "    - functional: a module(usually imported into the F namespace by convention) which contains activation functions, loss functions, etc, as well as non-stateful versions of layers such as convolutional and linear layers.\n",
    "- torch.optim: Contains optimizers such as SGD, which update the weights of Parameter during the backward step\n",
    "- Dataset: An abstract interface of objects with a __len__ and a __getitem__, including classes provided with Pytorch such as TensorDataset\n",
    "- DataLoader: Takes any Dataset and creates an iterator which returns batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "238.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
