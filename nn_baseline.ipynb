{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此代码的tensorflow的版本为1.3.1,deepctr的版本为0.7.4，这个无法在tensorflow2以及最新的deepctr上运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ksama\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Ksama\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Ksama\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Ksama\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Ksama\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Ksama\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING:root:\n",
      "DeepCTR version 0.8.0 detected. Your version is 0.7.4.\n",
      "Use `pip install -U deepctr` to upgrade.Changelog: https://github.com/shenweichen/DeepCTR/releases/tag/v0.8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gc\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from deepctr.models import *\n",
    "from deepctr.inputs import  SparseFeat, DenseFeat, get_feature_names\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau,LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\" #指定GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem(df):\n",
    "    starttime = time.time()\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if pd.isnull(c_min) or pd.isnull(c_max):\n",
    "                continue\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('-- Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction),time spend:{:2.2f} min'.format(end_mem,\n",
    "                                                                                                           100*(start_mem-end_mem)/start_mem,\n",
    "                                                                                                           (time.time()-starttime)/60))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#相关参数\n",
    "weights_path = './fibinet_base.h5'\n",
    "learning_rate = 1e-3\n",
    "batch = 8192*4\n",
    "n_epoch=100\n",
    "embedding_dim = 8 #embedding维度一般来说越大越好，但是维度越大跑起来越慢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df = pd.read_csv('../data/train_data.csv',sep='|')\n",
    "train_df = train_df.sample(frac=0.5).reset_index(drop=True)\n",
    "test_df = pd.read_csv('../data/test_data_A.csv',sep='|')\n",
    "df = pd.concat([train_df,test_df],axis=0)\n",
    "test_id = test_df['id'].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df=df.replace([np.inf, -np.inf],0)\n",
    "df=df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [00:55<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mem. usage decreased to 3287.04 Mb (57.7% reduction),time spend:0.16 min\n"
     ]
    }
   ],
   "source": [
    "#处理类别特征\n",
    "cate_cols = ['city_rank','creat_type_cd','dev_id','device_size','gender','indu_name','inter_type_cd','residence','slot_id','net_type','task_id','adv_id','adv_prim_id','age','app_first_class','app_second_class','career','city','consume_purchase','uid','dev_id','tags']\n",
    "\n",
    "for f in tqdm(cate_cols):\n",
    "    map_dict = dict(zip(df[f].unique(), range(df[f].nunique())))\n",
    "    df[f] = df[f].map(map_dict).fillna(-1).astype('int32')\n",
    "    df[f + '_count'] = df[f].map(df[f].value_counts())\n",
    "df = reduce_mem(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "['task_id', 'adv_id', 'creat_type_cd', 'adv_prim_id', 'inter_type_cd', 'slot_id', 'spread_app_id', 'tags', 'app_first_class', 'app_second_class', 'age', 'city', 'city_rank', 'device_name', 'device_size', 'career', 'gender', 'net_type', 'residence', 'his_app_size', 'his_on_shelf_time', 'app_score', 'emui_dev', 'list_time', 'device_price', 'up_life_duration', 'up_membership_grade', 'membership_life_duration', 'consume_purchase', 'communication_avgonline_30d', 'indu_name', 'id', 'city_rank_count', 'creat_type_cd_count', 'dev_id_count', 'device_size_count', 'gender_count', 'indu_name_count', 'inter_type_cd_count', 'residence_count', 'slot_id_count', 'net_type_count', 'task_id_count', 'adv_id_count', 'adv_prim_id_count', 'age_count', 'app_first_class_count', 'app_second_class_count', 'career_count', 'city_count', 'consume_purchase_count', 'uid_count', 'tags_count']\n"
     ]
    }
   ],
   "source": [
    "drop_fea = ['pt_d','label','communication_onlinerate','index','uid','dev_id']\n",
    "feature= [x for x in df.columns if x not in drop_fea]\n",
    "print(len(feature))\n",
    "print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse_feature: ['city_rank', 'creat_type_cd', 'dev_id', 'device_size', 'gender', 'indu_name', 'inter_type_cd', 'residence', 'slot_id', 'net_type', 'task_id', 'adv_id', 'adv_prim_id', 'age', 'app_first_class', 'app_second_class', 'career', 'city', 'consume_purchase', 'uid', 'dev_id', 'tags']\n",
      "dense_feature: ['spread_app_id', 'device_name', 'his_app_size', 'his_on_shelf_time', 'app_score', 'emui_dev', 'list_time', 'device_price', 'up_life_duration', 'up_membership_grade', 'membership_life_duration', 'communication_avgonline_30d', 'id', 'city_rank_count', 'creat_type_cd_count', 'dev_id_count', 'device_size_count', 'gender_count', 'indu_name_count', 'inter_type_cd_count', 'residence_count', 'slot_id_count', 'net_type_count', 'task_id_count', 'adv_id_count', 'adv_prim_id_count', 'age_count', 'app_first_class_count', 'app_second_class_count', 'career_count', 'city_count', 'consume_purchase_count', 'uid_count', 'tags_count']\n"
     ]
    }
   ],
   "source": [
    "sparse_features = cate_cols\n",
    "dense_features = [x for x in df.columns if x not in drop_fea+cate_cols] #这里的dense_feature可以把树模型的特征加进来\n",
    "print('sparse_feature: {}'.format(sparse_features))\n",
    "print('dense_feature: {}'.format(dense_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 19s\n"
     ]
    }
   ],
   "source": [
    "#对dense_features进行归一化\n",
    "mms = MinMaxScaler(feature_range=(0,1))\n",
    "df[dense_features] = mms.fit_transform(df[dense_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=df[feat].nunique(),embedding_dim=embedding_dim)\n",
    "                       for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,)\n",
    "                      for feat in dense_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = df[df[\"pt_d\"]==8].copy().reset_index()\n",
    "train_df = df[df[\"pt_d\"]<8].reset_index()\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_feature_columns = fixlen_feature_columns\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "online_train_model_input = {name:train_df[name].values for name in feature_names}\n",
    "online_test_model_input = {name:test_df[name].values for name in feature_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_category_focal_loss2(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    focal loss for multi category of multi label problem\n",
    "    适用于多分类或多标签问题的focal loss\n",
    "    alpha控制真值y_true为1/0时的权重\n",
    "        1的权重为alpha, 0的权重为1-alpha\n",
    "    当你的模型欠拟合，学习存在困难时，可以尝试适用本函数作为loss\n",
    "    当模型过于激进(无论何时总是倾向于预测出1),尝试将alpha调小\n",
    "    当模型过于惰性(无论何时总是倾向于预测出0,或是某一个固定的常数,说明没有学到有效特征)\n",
    "        尝试将alpha调大,鼓励模型进行预测出1。\n",
    "    Usage:\n",
    "     model.compile(loss=[multi_category_focal_loss2(alpha=0.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    epsilon = 1.e-7\n",
    "    gamma = float(gamma)\n",
    "    alpha = tf.constant(alpha, dtype=tf.float32)\n",
    "\n",
    "    def multi_category_focal_loss2_fixed(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "        alpha_t = y_true * alpha + (tf.ones_like(y_true) - y_true) * (1 - alpha)\n",
    "        y_t = tf.multiply(y_true, y_pred) + tf.multiply(1 - y_true, 1 - y_pred)\n",
    "        ce = -tf.log(y_t)\n",
    "        weight = tf.pow(tf.subtract(1., y_t), gamma)\n",
    "        fl = tf.multiply(tf.multiply(weight, ce), alpha_t)\n",
    "        loss = tf.reduce_mean(fl)\n",
    "        return loss\n",
    "\n",
    "    return multi_category_focal_loss2_fixed\n",
    "\n",
    "def auroc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ksama\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ksama\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ksama\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\deepctr\\layers\\utils.py:167: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ksama\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\deepctr\\layers\\utils.py:167: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ksama\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ksama\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-23-827a21ce0d36>:33: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-23-827a21ce0d36>:33: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16762852 samples, validate on 4190714 samples\n",
      "WARNING:tensorflow:From C:\\Users\\Ksama\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ksama\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16762852/16762852 [==============================] - 307s 18us/sample - loss: 0.0122 - auroc: 0.6739 - val_loss: 0.0065 - val_auroc: 0.7866\n",
      "Epoch 2/100\n",
      "16762852/16762852 [==============================] - 258s 15us/sample - loss: 0.0065 - auroc: 0.7822 - val_loss: 0.0064 - val_auroc: 0.7993\n",
      "Epoch 3/100\n",
      "16762852/16762852 [==============================] - 255s 15us/sample - loss: 0.0065 - auroc: 0.7898 - val_loss: 0.0063 - val_auroc: 0.8039\n",
      "Epoch 4/100\n",
      "16762852/16762852 [==============================] - 258s 15us/sample - loss: 0.0065 - auroc: 0.7936 - val_loss: 0.0063 - val_auroc: 0.8064\n",
      "Epoch 5/100\n",
      "16762852/16762852 [==============================] - 262s 16us/sample - loss: 0.0065 - auroc: 0.7965 - val_loss: 0.0063 - val_auroc: 0.8075\n",
      "Epoch 6/100\n",
      "16762852/16762852 [==============================] - 261s 16us/sample - loss: 0.0065 - auroc: 0.7990 - val_loss: 0.0063 - val_auroc: 0.8080\n",
      "Epoch 7/100\n",
      "16762852/16762852 [==============================] - 261s 16us/sample - loss: 0.0065 - auroc: 0.8005 - val_loss: 0.0063 - val_auroc: 0.8084\n",
      "Epoch 8/100\n",
      "16762852/16762852 [==============================] - 255s 15us/sample - loss: 0.0065 - auroc: 0.8024 - val_loss: 0.0063 - val_auroc: 0.8086\n",
      "Epoch 9/100\n",
      "16762852/16762852 [==============================] - 257s 15us/sample - loss: 0.0065 - auroc: 0.8038 - val_loss: 0.0063 - val_auroc: 0.8093\n",
      "Epoch 10/100\n",
      "16744448/16762852 [============================>.] - ETA: 0s - loss: 0.0065 - auroc: 0.8055\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "16762852/16762852 [==============================] - 257s 15us/sample - loss: 0.0065 - auroc: 0.8055 - val_loss: 0.0063 - val_auroc: 0.8091\n",
      "Epoch 11/100\n",
      "16744448/16762852 [============================>.] - ETA: 0s - loss: 0.0065 - auroc: 0.8073\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
      "16762852/16762852 [==============================] - 263s 16us/sample - loss: 0.0065 - auroc: 0.8073 - val_loss: 0.0064 - val_auroc: 0.8094\n",
      "Epoch 12/100\n",
      "16744448/16762852 [============================>.] - ETA: 0s - loss: 0.0065 - auroc: 0.8094\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0007290000503417104.\n",
      "16762852/16762852 [==============================] - 262s 16us/sample - loss: 0.0065 - auroc: 0.8094 - val_loss: 0.0065 - val_auroc: 0.8092\n",
      "Epoch 13/100\n",
      "16744448/16762852 [============================>.] - ETA: 0s - loss: 0.0065 - auroc: 0.8116\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0006561000715009868.\n",
      "16762852/16762852 [==============================] - 259s 15us/sample - loss: 0.0065 - auroc: 0.8116 - val_loss: 0.0065 - val_auroc: 0.8089\n",
      "Epoch 14/100\n",
      " 2785280/16762852 [===>..........................] - ETA: 3:08 - loss: 0.0063 - auroc: 0.8234"
     ]
    }
   ],
   "source": [
    "plateau = ReduceLROnPlateau(monitor=\"val_auroc\", verbose=1, mode='max', factor=0.3, patience=5)\n",
    "early_stopping = EarlyStopping(monitor='val_auroc', patience=9, mode='max')\n",
    "checkpoint = ModelCheckpoint(weights_path,\n",
    "                             monitor='val_auroc',\n",
    "                             verbose=0,\n",
    "                             mode='max',\n",
    "                             save_best_only=True)\n",
    "\n",
    "model = FiBiNET(linear_feature_columns,dnn_feature_columns,task='binary',dnn_dropout=0.1,dnn_hidden_units=(512, 128),)\n",
    "\n",
    "opt = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=opt,\n",
    "              #loss='binary_crossentropy',\n",
    "              loss = multi_category_focal_loss2(alpha=0.1, gamma=2),\n",
    "              metrics=[auroc], )\n",
    "\n",
    "history = model.fit(online_train_model_input, train_df['label'].values,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping,plateau,checkpoint],shuffle=True,\n",
    "                    batch_size=batch, epochs=n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(weights_path)\n",
    "y_pre = model.predict(online_test_model_input, batch_size=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()\n",
    "res['id'] = test_id\n",
    "res['probability'] = y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('./fibinet_base.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
