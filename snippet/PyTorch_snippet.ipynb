{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础\n",
    "\n",
    "\n",
    "1. https://zhuanlan.zhihu.com/p/83172023\n",
    "2. https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#我们自己定义的变量，我们称之为叶子张量(leaf Tensor)，而基于叶子节点得到的中间或最终变量则可称之为结果张量\n",
    "#这里X是leaf Tensor、其他的都是结果张量\n",
    "x=torch.tensor([[1.,2.,3.],[4.,5.,6.]],requires_grad=True)\n",
    "y=x+2\n",
    "z=y*y*3\n",
    "out=z.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.data\n",
    "x.requires_grad\n",
    "print(x.grad)\n",
    "print(x.grad_fn)\n",
    "print(x.is_leaf)\n",
    "\n",
    "y.data\n",
    "y.requires_grad\n",
    "print(y.grad)\n",
    "print(y.grad_fn)\n",
    "print(y.is_leaf)\n",
    "\n",
    "z.data\n",
    "z.requires_grad\n",
    "print(z.grad)\n",
    "print(z.grad_fn)\n",
    "print(z.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()\n",
    "x.grad\n",
    "#只能对leaf Tensor求梯度，下面代码会报错\n",
    "y.grad\n",
    "\n",
    "#梯度是累加的。可以发现我们的w初始为-0.8820,计算出来的导数为-47.0557 第2行是backward计算的，第3行是我们自己计算的。但是发现下面的第4、5行和想象的不一样，手动计算的好像没问题还是-47.0557，但为什么backward计算的变成了-94.1115,似乎是-47.0557*2,其实是因为pytorch设计就是这样的，它的梯度默认会保存累加，所以这次的梯度是我们这次的加上上一次的。讲到这里，估计也就明白了为什么我们在进行梯度下降的时候需要用optimizer.zero_grad()了，就是要清空梯度。\n",
    "x = torch.tensor([2])\n",
    "y = torch.tensor([10])\n",
    "loss = torch.nn.MSELoss()\n",
    "w = torch.randn(1,requires_grad=True)\n",
    "for i in range(10):\n",
    "    print('w = ',w)\n",
    "    y_ = w*x\n",
    "    l = loss(y,y_)\n",
    "    print('loss = ', l)\n",
    "    l.backward()\n",
    "    print('w grad = ',w.grad)\n",
    "\n",
    "#上面的例子只是计算了梯度，并没有进行梯度更新。下面加入更新梯度。（因为这里我重新定义了w，相当于清空了梯度，所以w的梯度不会累加了）\n",
    "w = torch.randn(1,requires_grad=True)\n",
    "# print(w)\n",
    "for i in range(10):\n",
    "    print('w = ',w)\n",
    "    y_ = w*x\n",
    "    l = loss(y,y_)\n",
    "    print('loss = ', l)\n",
    "    l.backward()\n",
    "    print('w grad = ',w.grad)\n",
    "    w = torch.tensor(w - 0.1 * w.grad.data, requires_grad=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
